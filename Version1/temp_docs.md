# URL: https://langchain-ai.github.io/langgraph/tutorials/get-started/1-build-basic-chatbot/
**Depth**: 0

# Build a basic chatbot In this tutorial, you will build a basic chatbot. This chatbot is the basis for the following series of tutorials where you will progressively add more sophisticated capabilities, and be introduced to key LangGraph concepts along the way. Let’s dive in! 🌟 ## Prerequisites Before you start this tutorial, ensure you have access to a LLM that supports tool-calling features, such as [OpenAI](https://platform.openai.com/api-keys), [Anthropic](https://console.anthropic.com/settings/keys), or [Google Gemini](https://ai.google.dev/gemini-api/docs/api-key). ## 1. Install packages Install the required packages: ```bash pip install -U langgraph langsmith ``` Tip: Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph. For more information on how to get started, see [LangSmith docs](https://docs.smith.langchain.com). ## 2. Create a `StateGraph` Now you can create a basic chatbot using LangGraph. This chatbot will respond directly to user messages. Start by creating a `StateGraph`. A `StateGraph` object defines the structure of our chatbot as a "state machine". We'll add `nodes` to represent the llm and functions our chatbot can call and `edges` to specify how the bot should transition between these functions. API Reference: [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) | [START](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START) | [END](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.END) | [add_messages](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages) ```python from typing import Annotated from typing_extensions import TypedDict from langgraph.graph import StateGraph, START, END from langgraph.graph.message import add_messages class State(TypedDict): # Messages have the type "list". The `add_messages` function # in the annotation defines how this state key should be updated # (in this case, it appends messages to the list, rather than overwriting them) messages: Annotated[list, add_messages] graph_builder = StateGraph(State) ``` Our graph can now handle two key tasks: 1. Each `node` can receive the current `State` as input and output an update to the state. 2. Updates to `messages` will be appended to the existing list rather than overwriting it, thanks to the prebuilt [`add_messages`](https://langchain-ai.github.io/langgraph/reference/graphs/?h=add+messages#add_messages) function used with the `Annotated` syntax. Concept: When defining a graph, the first step is to define its `State`. The `State` includes the graph's schema and [reducer functions](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers) that handle state updates. In our example, `State` is a `TypedDict` with one key: `messages`. The [`add_messages`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages) reducer function is used to append new messages to the list instead of overwriting it. Keys without a reducer annotation will overwrite previous values. To learn more about state, reducers, and related concepts, see [LangGraph reference docs](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages). ## 3. Add a node Next, add a "`chatbot`" node. **Nodes** represent units of work and are typically regular Python functions. Let's first select a chat model: **OpenAI** ```bash pip install -U "langchain[openai]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["OPENAI_API_KEY"] = "sk-..." llm = init_chat_model("openai:gpt-4.1") ``` 👉 Read the [OpenAI integration docs](https://python.langchain.com/docs/integrations/chat/openai/) **Anthropic** ```bash pip install -U "langchain[anthropic]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["ANTHROPIC_API_KEY"] = "sk-..." llm = init_chat_model("anthropic:claude-3-5-sonnet-latest") ``` 👉 Read the [Anthropic integration docs](https://python.langchain.com/docs/integrations/chat/anthropic/) **Azure** ```bash pip install -U "langchain[openai]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["AZURE_OPENAI_API_KEY"] = "..." os.environ["AZURE_OPENAI_ENDPOINT"] = "..." os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview" llm = init_chat_model( "azure_openai:gpt-4.1", azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"], ) ``` 👉 Read the [Azure integration docs](https://python.langchain.com/docs/integrations/chat/azure_chat_openai/) **Google Gemini** ```bash pip install -U "langchain[google-genai]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["GOOGLE_API_KEY"] = "..." llm = init_chat_model("google_genai:gemini-2.0-flash") ``` 👉 Read the [Google GenAI integration docs](https://python.langchain.com/docs/integrations/chat/google_generative_ai/) **AWS Bedrock** ```bash pip install -U "langchain[aws]" ``` ```python from langchain.chat_models import init_chat_model # Follow the steps here to configure your credentials: # https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html llm = init_chat_model( "anthropic.claude-3-5-sonnet-20240620-v1:0", model_provider="bedrock_converse", ) ``` 👉 Read the [AWS Bedrock integration docs](https://python.langchain.com/docs/integrations/chat/bedrock/) We can now incorporate the chat model into a simple node: ```python def chatbot(state: State): return {"messages": [llm.invoke(state["messages"])]} # The first argument is the unique node name # The second argument is the function or object that will be called whenever # the node is used. graph_builder.add_node("chatbot", chatbot) ``` Notice how the `chatbot` node function takes the current `State` as input and returns a dictionary containing an updated `messages` list under the key "messages". This is the basic pattern for all LangGraph node functions. The `add_messages` function in our `State` will append the LLM's response messages to whatever messages are already in the state. ## 4. Add an `entry` point Add an `entry` point to tell the graph **where to start its work** each time it is run: ```python graph_builder.add_edge(START, "chatbot") ``` ## 5. Add an `exit` point Add an `exit` point to indicate **where the graph should finish execution**. This is helpful for more complex flows, but even in a simple graph like this, adding an end node improves clarity. ```python graph_builder.add_edge("chatbot", END) ``` This tells the graph to terminate after running the chatbot node. ## 6. Compile the graph Before running the graph, we'll need to compile it. We can do so by calling `compile()` on the graph builder. This creates a `CompiledStateGraph` we can invoke on our state. ```python graph = graph_builder.compile() ``` ## 7. Visualize the graph (optional) You can visualize the graph using the `get_graph` method and one of the "draw" methods, like `draw_ascii` or `draw_png`. The `draw` methods each require additional dependencies. ```python from IPython.display import Image, display try: display(Image(graph.get_graph().draw_mermaid_png())) except Exception: # This requires some extra dependencies and is optional pass ``` ![basic chatbot diagram](https://langchain-ai.github.io/langgraph/tutorials/get-started/basic-chatbot.png) ## 8. Run the chatbot Now run the chatbot! Tip: You can exit the chat loop at any time by typing `quit`, `exit`, or `q`. ```python def stream_graph_updates(user_input: str): for event in graph.stream({"messages": [{"role": "user", "content": user_input}]}): for value in event.values(): print("Assistant:", value["messages"][-1].content) while True: try: user_input = input("User: ") if user_input.lower() in ["quit", "exit", "q"]: print("Goodbye!") break stream_graph_updates(user_input) except: # fallback if input() is not available user_input = "What do you know about LangGraph?" print("User: " + user_input) stream_graph_updates(user_input) break ``` ```text Assistant: LangGraph is a library designed to help build stateful multi-agent applications using language models. It provides tools for creating workflows and state machines to coordinate multiple AI agents or language model interactions. LangGraph is built on top of LangChain, leveraging its components while adding graph-based coordination capabilities. It's particularly useful for developing more complex, stateful AI applications that go beyond simple query-response interactions. Goodbye! ``` **Congratulations!** You've built your first chatbot using LangGraph. This bot can engage in basic conversation by taking user input and generating responses using an LLM. You can inspect a [LangSmith Trace](https://smith.langchain.com/public/7527e308-9502-4894-b347-f34385740d5a/r) for the call above. Below is the full code for this tutorial: API Reference: [init_chat_model](https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) | [START](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START) | [END](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.END) | [add_messages](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages) ```python from typing import Annotated from langchain.chat_models import init_chat_model from typing_extensions import TypedDict from langgraph.graph import StateGraph, START, END from langgraph.graph.message import add_messages class State(TypedDict): messages: Annotated[list, add_messages] graph_builder = StateGraph(State) llm = init_chat_model("anthropic:claude-3-5-sonnet-latest") def chatbot(state: State): return {"messages": [llm.invoke(state["messages"])]} # The first argument is the unique node name # The second argument is the function or object that will be called whenever # the node is used. graph_builder.add_node("chatbot", chatbot) graph_builder.add_edge(START, "chatbot") graph_builder.add_edge("chatbot", END) graph = graph_builder.compile() ``` ## Next steps You may have noticed that the bot's knowledge is limited to what's in its training data. In the next part, we'll [add a web search tool](https://langchain-ai.github.io/langgraph/tutorials/get-started/2-add-tools/) to expand the bot's knowledge and make it more capable.

---

# URL: https://langchain-ai.github.io/langgraph
**Depth**: 1

# LangGraph LangGraph is a low-level orchestration framework for building, managing, and deploying long-running, stateful agents. ## Get started Install LangGraph: ```bash pip install -U langgraph ``` Then, create an agent using prebuilt components: ```python # pip install -qU "langchain[anthropic]" to call the model from langgraph.prebuilt import create_react_agent def get_weather(city: str) -> str: """Get weather for a given city.""" return f"It's always sunny in {city}!" agent = create_react_agent( model="anthropic:claude-3-7-sonnet-latest", tools=[get_weather], prompt="You are a helpful assistant" ) # Run the agent agent.invoke( {"messages": [{"role": "user", "content": "what is the weather in sf"}]} ) ``` For more information, see the [Quickstart](https://langchain-ai.github.io/langgraph/agents/agents/). Or, to learn how to build an [agent workflow](https://langchain-ai.github.io/langgraph/concepts/low_level/) with a customizable architecture, long-term memory, and other complex task handling, see the [LangGraph basics tutorials](https://langchain-ai.github.io/langgraph/tutorials/get-started/1-build-basic-chatbot/). ## Core benefits LangGraph provides low-level supporting infrastructure for *any* long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits: * [Durable execution](https://langchain-ai.github.io/langgraph/concepts/durable_execution/): Build agents that persist through failures and can run for extended periods, automatically resuming from exactly where they left off. * [Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/): Seamlessly incorporate human oversight by inspecting and modifying agent state at any point during execution. * [Comprehensive memory](https://langchain-ai.github.io/langgraph/concepts/memory/): Create truly stateful agents with both short-term working memory for ongoing reasoning and long-term persistent memory across sessions. * [Debugging with LangSmith](http://www.langchain.com/langsmith): Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics. * [Production-ready deployment](https://langchain-ai.github.io/langgraph/concepts/deployment_options/): Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows. ## LangGraph’s ecosystem While LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with: * [LangSmith](http://www.langchain.com/langsmith) — Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time. * [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/) — Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams — and iterate quickly with visual prototyping in [LangGraph Studio](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/). * [LangChain](https://python.langchain.com/docs/introduction/) – Provides integrations and composable components to streamline LLM application development. > Note > > Looking for the JS version of LangGraph? See the [JS repo](https://github.com/langchain-ai/langgraphjs) and the [JS docs](https://langchain-ai.github.io/langgraphjs/). ## Additional resources * [Guides](https://langchain-ai.github.io/langgraph/how-tos/): Quick, actionable code snippets for topics such as streaming, adding memory & persistence, and design patterns (e.g. branching, subgraphs, etc.). * [Reference](https://langchain-ai.github.io/langgraph/reference/graphs/): Detailed reference on core classes, methods, how to use the graph and checkpointing APIs, and higher-level prebuilt components. * [Examples](https://langchain-ai.github.io/langgraph/tutorials/overview/): Guided examples on getting started with LangGraph. * [LangChain Academy](https://academy.langchain.com/courses/intro-to-langgraph): Learn the basics of LangGraph in our free, structured course. * [Templates](https://langchain-ai.github.io/langgraph/concepts/template_applications/): Pre-built reference apps for common agentic workflows (e.g. ReAct agent, memory, retrieval etc.) that can be cloned and adapted. * [Case studies](https://www.langchain.com/built-with-langgraph): Hear how industry leaders use LangGraph to ship AI applications at scale. ## Acknowledgements LangGraph is inspired by [Pregel](https://research.google/pubs/pub37252/) and [Apache Beam](https://beam.apache.org/). The public interface draws inspiration from [NetworkX](https://networkx.org/documentation/latest/). LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.

---

# URL: https://langchain-ai.github.io/langgraph/tutorials/get-started/1-build-basic-chatbot
**Depth**: 1

# Build a basic chatbot In this tutorial, you will build a basic chatbot. This chatbot is the basis for the following series of tutorials where you will progressively add more sophisticated capabilities, and be introduced to key LangGraph concepts along the way. Let’s dive in! 🌟 ## Prerequisites Before you start this tutorial, ensure you have access to a LLM that supports tool-calling features, such as [OpenAI](https://platform.openai.com/api-keys), [Anthropic](https://console.anthropic.com/settings/keys), or [Google Gemini](https://ai.google.dev/gemini-api/docs/api-key). ## 1. Install packages Install the required packages: ```bash pip install -U langgraph langsmith ``` Tip Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph. For more information on how to get started, see [LangSmith docs](https://docs.smith.langchain.com). ## 2. Create a `StateGraph` Now you can create a basic chatbot using LangGraph. This chatbot will respond directly to user messages. Start by creating a `StateGraph`. A `StateGraph` object defines the structure of our chatbot as a "state machine". We'll add `nodes` to represent the llm and functions our chatbot can call and `edges` to specify how the bot should transition between these functions. ```python from typing import Annotated from typing_extensions import TypedDict from langgraph.graph import StateGraph, START, END from langgraph.graph.message import add_messages class State(TypedDict): # Messages have the type "list". The `add_messages` function # in the annotation defines how this state key should be updated # (in this case, it appends messages to the list, rather than overwriting them) messages: Annotated[list, add_messages] graph_builder = StateGraph(State) ``` Our graph can now handle two key tasks: 1. Each `node` can receive the current `State` as input and output an update to the state. 2. Updates to `messages` will be appended to the existing list rather than overwriting it, thanks to the prebuilt `add_messages` function used with the `Annotated` syntax. Concept When defining a graph, the first step is to define its `State`. The `State` includes the graph's schema and [reducer functions](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers) that handle state updates. In our example, `State` is a `TypedDict` with one key: `messages`. The `add_messages` reducer function is used to append new messages to the list instead of overwriting it. Keys without a reducer annotation will overwrite previous values. To learn more about state, reducers, and related concepts, see [LangGraph reference docs](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages). ## 3. Add a node Next, add a "`chatbot`" node. **Nodes** represent units of work and are typically regular Python functions. Let's first select a chat model:  OpenAI ```bash pip install -U "langchain[openai]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["OPENAI_API_KEY"] = "sk-..." llm = init_chat_model("openai:gpt-4.1") ``` 👉 Read the [OpenAI integration docs](https://python.langchain.com/docs/integrations/chat/openai/)  Anthropic ```bash pip install -U "langchain[anthropic]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["ANTHROPIC_API_KEY"] = "sk-..." llm = init_chat_model("anthropic:claude-3-5-sonnet-latest") ``` 👉 Read the [Anthropic integration docs](https://python.langchain.com/docs/integrations/chat/anthropic/)  Azure ```bash pip install -U "langchain[openai]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["AZURE_OPENAI_API_KEY"] = "..." os.environ["AZURE_OPENAI_ENDPOINT"] = "..." os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview" llm = init_chat_model( "azure_openai:gpt-4.1", azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"], ) ``` 👉 Read the [Azure integration docs](https://python.langchain.com/docs/integrations/chat/azure_chat_openai/)  Google Gemini ```bash pip install -U "langchain[google-genai]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["GOOGLE_API_KEY"] = "..." llm = init_chat_model("google_genai:gemini-2.0-flash") ``` 👉 Read the [Google GenAI integration docs](https://python.langchain.com/docs/integrations/chat/google_generative_ai/)  AWS Bedrock ```bash pip install -U "langchain[aws]" ``` ```python from langchain.chat_models import init_chat_model # Follow the steps here to configure your credentials: # https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html llm = init_chat_model( "anthropic.claude-3-5-sonnet-20240620-v1:0", model_provider="bedrock_converse", ) ``` 👉 Read the [AWS Bedrock integration docs](https://python.langchain.com/docs/integrations/chat/bedrock/)  We can now incorporate the chat model into a simple node: ```python def chatbot(state: State): return {"messages": [llm.invoke(state["messages"])]} # The first argument is the unique node name # The second argument is the function or object that will be called whenever # the node is used. graph_builder.add_node("chatbot", chatbot) ``` **Notice** how the `chatbot` node function takes the current `State` as input and returns a dictionary containing an updated `messages` list under the key "messages". This is the basic pattern for all LangGraph node functions. The `add_messages` function in our `State` will append the LLM's response messages to whatever messages are already in the state. ## 4. Add an `entry` point Add an `entry` point to tell the graph **where to start its work** each time it is run: ```python graph_builder.add_edge(START, "chatbot") ``` ## 5. Add an `exit` point Add an `exit` point to indicate **where the graph should finish execution**. This is helpful for more complex flows, but even in a simple graph like this, adding an end node improves clarity. ```python graph_builder.add_edge("chatbot", END) ``` This tells the graph to terminate after running the chatbot node. ## 6. Compile the graph Before running the graph, we'll need to compile it. We can do so by calling `compile()` on the graph builder. This creates a `CompiledStateGraph` we can invoke on our state. ```python graph = graph_builder.compile() ``` ## 7. Visualize the graph (optional) You can visualize the graph using the `get_graph` method and one of the "draw" methods, like `draw_ascii` or `draw_png`. The `draw` methods each require additional dependencies. ```python from IPython.display import Image, display try: display(Image(graph.get_graph().draw_mermaid_png())) except Exception: # This requires some extra dependencies and is optional pass ``` ## 8. Run the chatbot Now run the chatbot! Tip You can exit the chat loop at any time by typing `quit`, `exit`, or `q`. ```python def stream_graph_updates(user_input: str): for event in graph.stream({"messages": [{"role": "user", "content": user_input}]}): for value in event.values(): print("Assistant:", value["messages"][-1].content) while True: try: user_input = input("User: ") if user_input.lower() in ["quit", "exit", "q"]: print("Goodbye!") break stream_graph_updates(user_input) except: # fallback if input() is not available user_input = "What do you know about LangGraph?" print("User: " + user_input) stream_graph_updates(user_input) break ``` ```text Assistant: LangGraph is a library designed to help build stateful multi-agent applications using language models. It provides tools for creating workflows and state machines to coordinate multiple AI agents or language model interactions. LangGraph is built on top of LangChain, leveraging its components while adding graph-based coordination capabilities. It's particularly useful for developing more complex, stateful AI applications that go beyond simple query-response interactions. Goodbye! ``` **Congratulations!** You've built your first chatbot using LangGraph. This bot can engage in basic conversation by taking user input and generating responses using an LLM. You can inspect a [LangSmith Trace](https://smith.langchain.com/public/7527e308-9502-4894-b347-f34385740d5a/r) for the call above. Below is the full code for this tutorial: ```python from typing import Annotated from langchain.chat_models import init_chat_model from typing_extensions import TypedDict from langgraph.graph import StateGraph, START, END from langgraph.graph.message import add_messages class State(TypedDict): messages: Annotated[list, add_messages] graph_builder = StateGraph(State) llm = init_chat_model("anthropic:claude-3-5-sonnet-latest") def chatbot(state: State): return {"messages": [llm.invoke(state["messages"])]} # The first argument is the unique node name # The second argument is the function or object that will be called whenever # the node is used. graph_builder.add_node("chatbot", chatbot) graph_builder.add_edge(START, "chatbot") graph_builder.add_edge("chatbot", END) graph = graph_builder.compile() ``` ## Next steps You may have noticed that the bot's knowledge is limited to what's in its training data. In the next part, we'll [add a web search tool](https://langchain-ai.github.io/langgraph/tutorials/get-started/2-add-tools/) to expand the bot's knowledge and make it more capable.

---

# URL: https://langchain-ai.github.io/langgraph/reference
**Depth**: 1

# Reference Welcome to the LangGraph reference docs! These pages detail the core interfaces you will use when building with LangGraph. Each section covers a different part of the ecosystem. **Tip**: If you are just getting started, see [LangGraph basics](https://langchain-ai.github.io/langgraph/concepts/why-langgraph/) for an introduction to the main concepts and usage patterns. ## LangGraph The core APIs for the LangGraph open source library. * [Graphs](https://langchain-ai.github.io/langgraph/reference/graphs/): Main graph abstraction and usage. * [Functional API](https://langchain-ai.github.io/langgraph/reference/func/): Functional programming interface for graphs. * [Pregel](https://langchain-ai.github.io/langgraph/reference/pregel/): Pregel-inspired computation model. * [Checkpointing](https://langchain-ai.github.io/langgraph/reference/checkpoints/): Saving and restoring graph state. * [Storage](https://langchain-ai.github.io/langgraph/reference/store/): Storage backends and options. * [Caching](https://langchain-ai.github.io/langgraph/reference/cache/): Caching mechanisms for performance. * [Types](https://langchain-ai.github.io/langgraph/reference/types/): Type definitions for graph components. * [Config](https://langchain-ai.github.io/langgraph/reference/config/): Configuration options. * [Errors](https://langchain-ai.github.io/langgraph/reference/errors/): Error types and handling. * [Constants](https://langchain-ai.github.io/langgraph/reference/constants/): Global constants. * [Channels](https://langchain-ai.github.io/langgraph/reference/channels/): Message passing and channels. ## Prebuilt components Higher-level abstractions for common workflows, agents, and other patterns. * [Agents](https://langchain-ai.github.io/langgraph/reference/agents/): Built-in agent patterns. * [Supervisor](https://langchain-ai.github.io/langgraph/reference/supervisor/): Orchestration and delegation. * [Swarm](https://langchain-ai.github.io/langgraph/reference/swarm/): Multi-agent collaboration. * [MCP Adapters](https://langchain-ai.github.io/langgraph/reference/mcp/): Integrations with external systems. ## LangGraph Platform Tools for deploying and connecting to the LangGraph Platform. * [CLI](https://langchain-ai.github.io/langgraph/cloud/reference/cli/): Command-line interface for building and deploying LangGraph Platform applications. * [Server API](https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref/): REST API for the LangGraph Server. * [SDK (Python)](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/): Python SDK for interacting with instances of the LangGraph Server. * [SDK (JS/TS)](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/js_ts_sdk_ref/): JavaScript/TypeScript SDK for interacting with instances of the LangGraph Server. * [RemoteGraph](https://langchain-ai.github.io/langgraph/reference/remote_graph/): `Pregel` abstraction for connecting to LangGraph Server instances. * [Environment variables](https://langchain-ai.github.io/langgraph/cloud/reference/env_var/): Supported configuration variables when deploying with the LangGraph Platform.

---

# URL: https://langchain-ai.github.io/langgraph/concepts/agentic_concepts
**Depth**: 1

# Agent Architectures Many LLM applications implement a particular control flow of steps before and / or after LLM calls. As an example, [RAG](https://github.com/langchain-ai/rag-from-scratch) performs retrieval of documents relevant to a user question, and passes those documents to an LLM in order to ground the model's response in the provided document context. Instead of hard-coding a fixed control flow, we sometimes want LLM systems that can pick their own control flow to solve more complex problems! This is one definition of an [agent](https://blog.langchain.dev/what-is-an-agent/): *an agent is a system that uses an LLM to decide the control flow of an application.* There are many ways that an LLM can control application: * An LLM can route between two potential paths * An LLM can decide which of many tools to call * An LLM can decide whether the generated answer is sufficient or more work is needed As a result, there are many different types of [agent architectures](https://blog.langchain.dev/what-is-a-cognitive-architecture/), which give an LLM varying levels of control. ## Router A router allows an LLM to select a single step from a specified set of options. This is an agent architecture that exhibits a relatively limited level of control because the LLM usually focuses on making a single decision and produces a specific output from a limited set of pre-defined options. Routers typically employ a few different concepts to achieve this. ### Structured Output Structured outputs with LLMs work by providing a specific format or schema that the LLM should follow in its response. This is similar to tool calling, but more general. While tool calling typically involves selecting and using predefined functions, structured outputs can be used for any type of formatted response. Common methods to achieve structured outputs include: 1. Prompt engineering: Instructing the LLM to respond in a specific format via the system prompt. 2. Output parsers: Using post-processing to extract structured data from LLM responses. 3. Tool calling: Leveraging built-in tool calling capabilities of some LLMs to generate structured outputs. Structured outputs are crucial for routing as they ensure the LLM's decision can be reliably interpreted and acted upon by the system. Learn more about [structured outputs in this how-to guide](https://python.langchain.com/docs/how_to/structured_output/). ## Tool-calling agent While a router allows an LLM to make a single decision, more complex agent architectures expand the LLM's control in two key ways: 1. Multi-step decision making: The LLM can make a series of decisions, one after another, instead of just one. 2. Tool access: The LLM can choose from and use a variety of tools to accomplish tasks. [ReAct](https://arxiv.org/abs/2210.03629) is a popular general purpose agent architecture that combines these expansions, integrating three core concepts. 1. [Tool calling](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#tool-calling): Allowing the LLM to select and use various tools as needed. 2. [Memory](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#memory): Enabling the agent to retain and use information from previous steps. 3. [Planning](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#planning): Empowering the LLM to create and follow multi-step plans to achieve goals. This architecture allows for more complex and flexible agent behaviors, going beyond simple routing to enable dynamic problem-solving with multiple steps. Unlike the original [paper](https://arxiv.org/abs/2210.03629), today's agents rely on LLMs' [tool calling](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#tool-calling) capabilities and operate on a list of [messages](https://langchain-ai.github.io/langgraph/concepts/low_level/#why-use-messages). In LangGraph, you can use the prebuilt [agent](https://langchain-ai.github.io/langgraph/agents/agents/#2-create-an-agent) to get started with tool-calling agents. ### Tool calling Tools are useful whenever you want an agent to interact with external systems. External systems (e.g., APIs) often require a particular input schema or payload, rather than natural language. When we bind an API, for example, as a tool, we give the model awareness of the required input schema. The model will choose to call a tool based upon the natural language input from the user and it will return an output that adheres to the tool's required schema. [Many LLM providers support tool calling](https://python.langchain.com/docs/integrations/chat/) and [tool calling interface](https://blog.langchain.dev/improving-core-tool-interfaces-and-docs-in-langchain/) in LangChain is simple: you can simply pass any Python `function` into `ChatModel.bind_tools(function)`. ### Memory [Memory](https://langchain-ai.github.io/langgraph/how-tos/memory/add-memory/) is crucial for agents, enabling them to retain and utilize information across multiple steps of problem-solving. It operates on different scales: 1. [Short-term memory](https://langchain-ai.github.io/langgraph/how-tos/memory/add-memory/#add-short-term-memory): Allows the agent to access information acquired during earlier steps in a sequence. 2. [Long-term memory](https://langchain-ai.github.io/langgraph/how-tos/memory/add-memory/#add-long-term-memory): Enables the agent to recall information from previous interactions, such as past messages in a conversation. LangGraph provides full control over memory implementation: * [`State`](https://langchain-ai.github.io/langgraph/concepts/low_level/#state): User-defined schema specifying the exact structure of memory to retain. * [`Checkpointer`](https://langchain-ai.github.io/langgraph/concepts/persistence/#checkpoints): Mechanism to store state at every step across different interactions within a session. * [`Store`](https://langchain-ai.github.io/langgraph/concepts/persistence/#memory-store): Mechanism to store user-specific or application-level data across sessions. This flexible approach allows you to tailor the memory system to your specific agent architecture needs. Effective memory management enhances an agent's ability to maintain context, learn from past experiences, and make more informed decisions over time. For a practical guide on adding and managing memory, see [Memory](https://langchain-ai.github.io/langgraph/how-tos/memory/add-memory/). ### Planning In a tool-calling [agent](https://langchain-ai.github.io/langgraph/agents/overview/#what-is-an-agent), an LLM is called repeatedly in a while-loop. At each step the agent decides which tools to call, and what the inputs to those tools should be. Those tools are then executed, and the outputs are fed back into the LLM as observations. The while-loop terminates when the agent decides it has enough information to solve the user request and it is not worth calling any more tools. ## Custom agent architectures While routers and tool-calling agents (like ReAct) are common, [customizing agent architectures](https://blog.langchain.dev/why-you-should-outsource-your-agentic-infrastructure-but-own-your-cognitive-architecture/) often leads to better performance for specific tasks. LangGraph offers several powerful features for building tailored agent systems: ### Human-in-the-loop Human involvement can significantly enhance agent reliability, especially for sensitive tasks. This can involve: * Approving specific actions * Providing feedback to update the agent's state * Offering guidance in complex decision-making processes Human-in-the-loop patterns are crucial when full automation isn't feasible or desirable. Learn more in our [human-in-the-loop guide](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/). ### Parallelization Parallel processing is vital for efficient multi-agent systems and complex tasks. LangGraph supports parallelization through its [Send](https://langchain-ai.github.io/langgraph/concepts/low_level/#send) API, enabling: * Concurrent processing of multiple states * Implementation of map-reduce-like operations * Efficient handling of independent subtasks For practical implementation, see our [map-reduce tutorial](https://langchain-ai.github.io/langgraph/how-tos/graph-api/#map-reduce-and-the-send-api) ### Subgraphs [Subgraphs](https://langchain-ai.github.io/langgraph/concepts/subgraphs/) are essential for managing complex agent architectures, particularly in [multi-agent systems](https://langchain-ai.github.io/langgraph/concepts/multi_agent/). They allow: * Isolated state management for individual agents * Hierarchical organization of agent teams * Controlled communication between agents and the main system Subgraphs communicate with the parent graph through overlapping keys in the state schema. This enables flexible, modular agent design. For implementation details, refer to our [subgraph how-to guide](https://langchain-ai.github.io/langgraph/how-tos/subgraph/). ### Reflection Reflection mechanisms can significantly improve agent reliability by: 1. Evaluating task completion and correctness 2. Providing feedback for iterative improvement 3. Enabling self-correction and learning While often LLM-based, reflection can also use deterministic methods. For instance, in coding tasks, compilation errors can serve as feedback. This approach is demonstrated in [this video using LangGraph for self-corrective code generation](https://www.youtube.com/watch?v=MvNdgmM7uyc). By leveraging these features, LangGraph enables the creation of sophisticated, task-specific agent architectures that can handle complex workflows, collaborate effectively, and continuously improve their performance.

---

# URL: https://langchain-ai.github.io/langgraph/concepts/why-langgraph
**Depth**: 1

# Overview LangGraph is built for developers who want to build powerful, adaptable AI agents. Developers choose LangGraph for: * **Reliability and controllability.** Steer agent actions with moderation checks and human-in-the-loop approvals. LangGraph persists context for long-running workflows, keeping your agents on course. * **Low-level and extensible.** Build custom agents with fully descriptive, low-level primitives free from rigid abstractions that limit customization. Design scalable multi-agent systems, with each agent serving a specific role tailored to your use case. * **First-class streaming support.** With token-by-token streaming and streaming of intermediate steps, LangGraph gives users clear visibility into agent reasoning and actions as they unfold in real time. ## Learn LangGraph basics To get acquainted with LangGraph's key concepts and features, complete the following LangGraph basics tutorials series: 1. [Build a basic chatbot](https://langchain-ai.github.io/langgraph/tutorials/get-started/1-build-basic-chatbot/) 2. [Add tools](https://langchain-ai.github.io/langgraph/tutorials/get-started/2-add-tools/) 3. [Add memory](https://langchain-ai.github.io/langgraph/tutorials/get-started/3-add-memory/) 4. [Add human-in-the-loop controls](https://langchain-ai.github.io/langgraph/tutorials/get-started/4-human-in-the-loop/) 5. [Customize state](https://langchain-ai.github.io/langgraph/tutorials/get-started/5-customize-state/) 6. [Time travel](https://langchain-ai.github.io/langgraph/tutorials/get-started/6-time-travel/) In completing this series of tutorials, you will build a support chatbot in LangGraph that can: * ✅ **Answer common questions** by searching the web * ✅ **Maintain conversation state** across calls * ✅ **Route complex queries** to a human for review * ✅ **Use custom state** to control its behavior * ✅ **Rewind and explore** alternative conversation paths

---

# URL: https://langchain-ai.github.io/langgraph/agents/agents
**Depth**: 1

# LangGraph quickstart This guide shows you how to set up and use LangGraph's **prebuilt**, **reusable** components, which are designed to help you construct agentic systems quickly and reliably. ## Prerequisites Before you start this tutorial, ensure you have the following: * An [Anthropic](https://console.anthropic.com/settings/keys) API key ## 1. Install dependencies If you haven't already, install LangGraph and LangChain: ``` pip install -U langgraph "langchain[anthropic]" ``` LangChain is installed so the agent can call the [model](https://python.langchain.com/docs/integrations/chat/). ## 2. Create an agent To create an agent, use `create_react_agent`: ```python from langgraph.prebuilt import create_react_agent def get_weather(city: str) -> str: """Get weather for a given city.""" return f"It's always sunny in {city}!" agent = create_react_agent( model="anthropic:claude-3-7-sonnet-latest", tools=[get_weather], prompt="You are a helpful assistant" ) # Run the agent agent.invoke( {"messages": [{"role": "user", "content": "what is the weather in sf"}]} ) ``` ## 3. Configure an LLM To configure an LLM with specific parameters, such as temperature, use `init_chat_model`: ```python from langchain.chat_models import init_chat_model from langgraph.prebuilt import create_react_agent model = init_chat_model( "anthropic:claude-3-7-sonnet-latest", temperature=0 ) agent = create_react_agent( model=model, tools=[get_weather], ) ``` For more information on how to configure LLMs, see [Models](https://langchain-ai.github.io/langgraph/agents/models/). ## 4. Add a custom prompt Prompts instruct the LLM how to behave. Add one of the following types of prompts: * **Static**: A string is interpreted as a **system message**. * **Dynamic**: A list of messages generated at **runtime**, based on input or configuration. ### Static prompt Define a fixed prompt string or list of messages: ```python from langgraph.prebuilt import create_react_agent agent = create_react_agent( model="anthropic:claude-3-7-sonnet-latest", tools=[get_weather], # A static prompt that never changes prompt="Never answer questions about the weather." ) agent.invoke( {"messages": [{"role": "user", "content": "what is the weather in sf"}]} ) ``` ### Dynamic prompt Define a function that returns a message list based on the agent's state and configuration: ```python from langchain_core.messages import AnyMessage from langchain_core.runnables import RunnableConfig from langgraph.prebuilt.chat_agent_executor import AgentState from langgraph.prebuilt import create_react_agent def prompt(state: AgentState, config: RunnableConfig) -> list[AnyMessage]: user_name = config["configurable"].get("user_name") system_msg = f"You are a helpful assistant. Address the user as {user_name}." return [{"role": "system", "content": system_msg}] + state["messages"] agent = create_react_agent( model="anthropic:claude-3-7-sonnet-latest", tools=[get_weather], prompt=prompt ) agent.invoke( {"messages": [{"role": "user", "content": "what is the weather in sf"}]}, config={"configurable": {"user_name": "John Smith"}} ) ``` 1. Dynamic prompts allow including non-message [context](https://langchain-ai.github.io/langgraph/agents/context/) when constructing an input to the LLM, such as: * Information passed at runtime, like a `user_id` or API credentials (using `config`). * Internal agent state updated during a multi-step reasoning process (using `state`). Dynamic prompts can be defined as functions that take `state` and `config` and return a list of messages to send to the LLM. For more information, see [Context](https://langchain-ai.github.io/langgraph/agents/context/). ## 5. Add memory To allow multi-turn conversations with an agent, you need to enable [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) by providing a `checkpointer` when creating an agent. At runtime, you need to provide a config containing `thread_id` — a unique identifier for the conversation (session): ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver checkpointer = InMemorySaver() agent = create_react_agent( model="anthropic:claude-3-7-sonnet-latest", tools=[get_weather], checkpointer=checkpointer ) # Run the agent config = {"configurable": {"thread_id": "1"}} sf_response = agent.invoke( {"messages": [{"role": "user", "content": "what is the weather in sf"}]}, config ) ny_response = agent.invoke( {"messages": [{"role": "user", "content": "what about new york?"}]}, config ) ``` When you enable the checkpointer, it stores agent state at every step in the provided checkpointer database (or in memory, if using `InMemorySaver`). Note that in the above example, when the agent is invoked the second time with the same `thread_id`, the original message history from the first conversation is automatically included, together with the new user input. For more information, see [Memory](https://langchain-ai.github.io/langgraph/how-tos/memory/add-memory/). ## 6. Configure structured output To produce structured responses conforming to a schema, use the `response_format` parameter. The schema can be defined with a `Pydantic` model or `TypedDict`. The result will be accessible via the `structured_response` field. ```python from pydantic import BaseModel from langgraph.prebuilt import create_react_agent class WeatherResponse(BaseModel): conditions: str agent = create_react_agent( model="anthropic:claude-3-7-sonnet-latest", tools=[get_weather], response_format=WeatherResponse ) response = agent.invoke( {"messages": [{"role": "user", "content": "what is the weather in sf"}]} ) response["structured_response"] ``` Structured output requires an additional call to the LLM to format the response according to the schema. ## Next steps * [Deploy your agent locally](https://langchain-ai.github.io/langgraph/tutorials/langgraph-platform/local-server/) * [Learn more about prebuilt agents](https://langchain-ai.github.io/langgraph/agents/overview/) * [LangGraph Platform quickstart](https://langchain-ai.github.io/langgraph/cloud/quick_start/)

---

# URL: https://langchain-ai.github.io/langgraph/concepts/template_applications
**Depth**: 1

# Template Applications Templates are open source reference applications designed to help you get started quickly when building with LangGraph. They provide working examples of common agentic workflows that can be customized to your needs. You can create an application from a template using the LangGraph CLI. Requirements: * Python >= 3.11 * [LangGraph CLI](https://langchain-ai.github.io/langgraph/cloud/reference/cli/): Requires langchain-cli[inmem] >= 0.1.58 ## Install the LangGraph CLI **Python:** ``` pip install "langgraph-cli[inmem]" --upgrade ``` Or via [`uv`](https://docs.astral.sh/uv/getting-started/installation/) (recommended): ``` uvx --from "langgraph-cli[inmem]" langgraph dev --help ``` **JS:** ``` npx @langchain/langgraph-cli --help ``` ## Available Templates | Template | Description | Python | JS/TS | | :------------------------- | :------------------------------------------------------------------------------------------------------ | :--------------------------------------------------------------------- | :-------------------------------------------------------------------- | | **New LangGraph Project** | A simple, minimal chatbot with memory. | [Repo](https://github.com/langchain-ai/new-langgraph-project) | [Repo](https://github.com/langchain-ai/new-langgraphjs-project) | | **ReAct Agent** | A simple agent that can be flexibly extended to many tools. | [Repo](https://github.com/langchain-ai/react-agent) | [Repo](https://github.com/langchain-ai/react-agent-js) | | **Memory Agent** | A ReAct-style agent with an additional tool to store memories for use across threads. | [Repo](https://github.com/langchain-ai/memory-agent) | [Repo](https://github.com/langchain-ai/memory-agent-js) | | **Retrieval Agent** | An agent that includes a retrieval-based question-answering system. | [Repo](https://github.com/langchain-ai/retrieval-agent-template) | [Repo](https://github.com/langchain-ai/retrieval-agent-template-js) | | **Data-Enrichment Agent** | An agent that performs web searches and organizes its findings into a structured format. | [Repo](https://github.com/langchain-ai/data-enrichment) | [Repo](https://github.com/langchain-ai/data-enrichment-js) | ## 🌱 Create a LangGraph App To create a new app from a template, use the `langgraph new` command. **Python:** ``` langgraph new ``` Or via [`uv`](https://docs.astral.sh/uv/getting-started/installation/) (recommended): ``` uvx --from "langgraph-cli[inmem]" langgraph new ``` **JS:** ``` npm create langgraph@latest ``` ## Next Steps Review the `README.md` file in the root of your new LangGraph app for more information about the template and how to customize it. After configuring the app properly and adding your API keys, you can start the app using the LangGraph CLI: **Python:** ``` langgraph dev ``` Or via [`uv`](https://docs.astral.sh/uv/getting-started/installation/) (recommended): ``` uvx --from "langgraph-cli[inmem]" --with-editable . langgraph dev ``` See the following guides for more information on how to deploy your app: * **[Launch Local LangGraph Server](https://langchain-ai.github.io/langgraph/tutorials/langgraph-platform/local-server/)**: This quick start guide shows how to start a LangGraph Server locally for the **ReAct Agent** template. The steps are similar for other templates. * **[Deploy to LangGraph Platform](https://langchain-ai.github.io/langgraph/cloud/quick_start/)**: Deploy your LangGraph app using LangGraph Platform.

---

# URL: https://langchain-ai.github.io/langgraph/tutorials/get-started/3-add-memory
**Depth**: 1

# Add memory The chatbot can now [use tools](https://langchain-ai.github.io/langgraph/tutorials/get-started/2-add-tools/) to answer user questions, but it does not remember the context of previous interactions. This limits its ability to have coherent, multi-turn conversations. LangGraph solves this problem through **persistent checkpointing**. If you provide a `checkpointer` when compiling the graph and a `thread_id` when calling your graph, LangGraph automatically saves the state after each step. When you invoke the graph again using the same `thread_id`, the graph loads its saved state, allowing the chatbot to pick up where it left off. We will see later that **checkpointing** is *much* more powerful than simple chat memory - it lets you save and resume complex state at any time for error recovery, human-in-the-loop workflows, time travel interactions, and more. But first, let's add checkpointing to enable multi-turn conversations. Note: This tutorial builds on [Add tools](https://langchain-ai.github.io/langgraph/tutorials/get-started/2-add-tools/). ## 1. Create a `MemorySaver` checkpointer Create a `MemorySaver` checkpointer: ```python from langgraph.checkpoint.memory import MemorySaver memory = MemorySaver() ``` This is in-memory checkpointer, which is convenient for the tutorial. However, in a production application, you would likely change this to use `SqliteSaver` or `PostgresSaver` and connect a database. ## 2. Compile the graph Compile the graph with the provided checkpointer, which will checkpoint the `State` as the graph works through each node: ```python graph = graph_builder.compile(checkpointer=memory) ``` ```python from IPython.display import Image, display try: display(Image(graph.get_graph().draw_mermaid_png())) except Exception: # This requires some extra dependencies and is optional pass ``` ## 3. Interact with your chatbot Now you can interact with your bot! 1. Pick a thread to use as the key for this conversation. ```python config = {"configurable": {"thread_id": "1"}} ``` 2. Call your chatbot: ```python user_input = "Hi there! My name is Will." # The config is the **second positional argument** to stream() or invoke()! events = graph.stream( {"messages": [{"role": "user", "content": user_input}]}, config, stream_mode="values", ) for event in events: event["messages"][-1].pretty_print() ``` ``` ================================ Human Message ================================= Hi there! My name is Will. ================================== Ai Message ================================== Hello Will! It's nice to meet you. How can I assist you today? Is there anything specific you'd like to know or discuss? ``` Note: The config was provided as the **second positional argument** when calling our graph. It importantly is *not* nested within the graph inputs (`{'messages': []}`). ## 4. Ask a follow up question Ask a follow up question: ```python user_input = "Remember my name?" # The config is the **second positional argument** to stream() or invoke()! events = graph.stream( {"messages": [{"role": "user", "content": user_input}]}, config, stream_mode="values", ) for event in events: event["messages"][-1].pretty_print() ``` ``` ================================ Human Message ================================= Remember my name? ================================== Ai Message ================================== Of course, I remember your name, Will. I always try to pay attention to important details that users share with me. Is there anything else you'd like to talk about or any questions you have? I'm here to help with a wide range of topics or tasks. ``` **Notice** that we aren't using an external list for memory: it's all handled by the checkpointer! You can inspect the full execution in this [LangSmith trace](https://smith.langchain.com/public/29ba22b5-6d40-4fbe-8d27-b369e3329c84/r) to see what's going on. Don't believe me? Try this using a different config. ```python # The only difference is we change the `thread_id` here to "2" instead of "1" events = graph.stream( {"messages": [{"role": "user", "content": user_input}]}, {"configurable": {"thread_id": "2"}}, stream_mode="values", ) for event in events: event["messages"][-1].pretty_print() ``` ``` ================================ Human Message ================================= Remember my name? ================================== Ai Message ================================== I apologize, but I don't have any previous context or memory of your name. As an AI assistant, I don't retain information from past conversations. Each interaction starts fresh. Could you please tell me your name so I can address you properly in this conversation? ``` **Notice** that the **only** change we've made is to modify the `thread_id` in the config. See this call's [LangSmith trace](https://smith.langchain.com/public/51a62351-2f0a-4058-91cc-9996c5561428/r) for comparison. ## 5. Inspect the state By now, we have made a few checkpoints across two different threads. But what goes into a checkpoint? To inspect a graph's `state` for a given config at any time, call `get_state(config)`. ```python snapshot = graph.get_state(config) snapshot ``` ``` StateSnapshot(values={'messages': [HumanMessage(content='Hi there! My name is Will.', additional_kwargs={}, response_metadata={}, id='8c1ca919-c553-4ebf-95d4-b59a2d61e078'), AIMessage(content="Hello Will! It's nice to meet you. How can I assist you today? Is there anything specific you'd like to know or discuss?", additional_kwargs={}, response_metadata={'id': 'msg_01WTQebPhNwmMrmmWojJ9KXJ', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 405, 'output_tokens': 32}}, id='run-58587b77-8c82-41e6-8a90-d62c444a261d-0', usage_metadata={'input_tokens': 405, 'output_tokens': 32, 'total_tokens': 437}), HumanMessage(content='Remember my name?', additional_kwargs={}, response_metadata={}, id='daba7df6-ad75-4d6b-8057-745881cea1ca'), AIMessage(content="Of course, I remember your name, Will. I always try to pay attention to important details that users share with me. Is there anything else you'd like to talk about or any questions you have? I'm here to help with a wide range of topics or tasks.", additional_kwargs={}, response_metadata={'id': 'msg_01E41KitY74HpENRgXx94vag', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 444, 'output_tokens': 58}}, id='run-ffeaae5c-4d2d-4ddb-bd59-5d5cbf2a5af8-0', usage_metadata={'input_tokens': 444, 'output_tokens': 58, 'total_tokens': 502})]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef7d06e-93e0-6acc-8004-f2ac846575d2'}}, metadata={'source': 'loop', 'writes': {'chatbot': {'messages': [AIMessage(content="Of course, I remember your name, Will. I always try to pay attention to important details that users share with me. Is there anything else you'd like to talk about or any questions you have? I'm here to help with a wide range of topics or tasks.", additional_kwargs={}, response_metadata={'id': 'msg_01E41KitY74HpENRgXx94vag', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 444, 'output_tokens': 58}}, id='run-ffeaae5c-4d2d-4ddb-bd59-5d5cbf2a5af8-0', usage_metadata={'input_tokens': 444, 'output_tokens': 58, 'total_tokens': 502})]}}, 'step': 4, 'parents': {}}, created_at='2024-09-27T19:30:10.820758+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef7d06e-859f-6206-8003-e1bd3c264b8f'}}, tasks=()) ``` ```python snapshot.next # (since the graph ended this turn, `next` is empty. If you fetch a state from within a graph invocation, next tells which node will execute next) ``` The snapshot above contains the current state values, corresponding config, and the `next` node to process. In our case, the graph has reached an `END` state, so `next` is empty. **Congratulations!** Your chatbot can now maintain conversation state across sessions thanks to LangGraph's checkpointing system. This opens up exciting possibilities for more natural, contextual interactions. LangGraph's checkpointing even handles **arbitrarily complex graph states**, which is much more expressive and powerful than simple chat memory. Check out the code snippet below to review the graph from this tutorial: ``` pip install -U "langchain[openai]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["OPENAI_API_KEY"] = "sk-..." llm = init_chat_model("openai:gpt-4.1") ``` 👉 Read the [OpenAI integration docs](https://python.langchain.com/docs/integrations/chat/openai/) ``` pip install -U "langchain[anthropic]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["ANTHROPIC_API_KEY"] = "sk-..." llm = init_chat_model("anthropic:claude-3-5-sonnet-latest") ``` 👉 Read the [Anthropic integration docs](https://python.langchain.com/docs/integrations/chat/anthropic/) ``` pip install -U "langchain[openai]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["AZURE_OPENAI_API_KEY"] = "..." os.environ["AZURE_OPENAI_ENDPOINT"] = "..." os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview" llm = init_chat_model( "azure_openai:gpt-4.1", azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"], ) ``` 👉 Read the [Azure integration docs](https://python.langchain.com/docs/integrations/chat/azure_chat_openai/) ``` pip install -U "langchain[google-genai]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["GOOGLE_API_KEY"] = "..." llm = init_chat_model("google_genai:gemini-2.0-flash") ``` 👉 Read the [Google GenAI integration docs](https://python.langchain.com/docs/integrations/chat/google_generative_ai/) ``` pip install -U "langchain[aws]" ``` ```python from langchain.chat_models import init_chat_model # Follow the steps here to configure your credentials: # https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html llm = init_chat_model( "anthropic.claude-3-5-sonnet-20240620-v1:0", model_provider="bedrock_converse", ) ``` 👉 Read the [AWS Bedrock integration docs](https://python.langchain.com/docs/integrations/chat/bedrock/) ```python from typing import Annotated from langchain.chat_models import init_chat_model from langchain_tavily import TavilySearch from langchain_core.messages import BaseMessage from typing_extensions import TypedDict from langgraph.checkpoint.memory import MemorySaver from langgraph.graph import StateGraph from langgraph.graph.message import add_messages from langgraph.prebuilt import ToolNode, tools_condition class State(TypedDict): messages: Annotated[list, add_messages] graph_builder = StateGraph(State) tool = TavilySearch(max_results=2) tools = [tool] llm_with_tools = llm.bind_tools(tools) def chatbot(state: State): return {"messages": [llm_with_tools.invoke(state["messages"])]} graph_builder.add_node("chatbot", chatbot) tool_node = ToolNode(tools=[tool]) graph_builder.add_node("tools", tool_node) graph_builder.add_conditional_edges( "chatbot", tools_condition, ) graph_builder.add_edge("tools", "chatbot") graph_builder.set_entry_point("chatbot") memory = MemorySaver() graph = graph_builder.compile(checkpointer=memory) ``` ## Next steps In the next tutorial, you will [add human-in-the-loop to the chatbot](https://langchain-ai.github.io/langgraph/tutorials/get-started/4-human-in-the-loop/) to handle situations where it may need guidance or verification before proceeding.

---

# URL: https://langchain-ai.github.io/langgraph/concepts/low_level
**Depth**: 1

# Graph API concepts At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 1. `State`: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a `TypedDict` or Pydantic `BaseModel`. 2. `Nodes`: Python functions that encode the logic of your agents. They receive the current `State` as input, perform some computation or side-effect, and return an updated `State`. 3. `Edges`: Python functions that determine which `Node` to execute next based on the current `State`. They can be conditional branches or fixed transitions. By composing `Nodes` and `Edges`, you can create complex, looping workflows that evolve the `State` over time. The real power, though, comes from how LangGraph manages that `State`. To emphasize: `Nodes` and `Edges` are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: *nodes do the work, edges tell what to do next*. LangGraph's underlying graph algorithm uses [message passing](https://en.wikipedia.org/wiki/Message_passing) to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's [Pregel](https://research.google/pubs/pregel-a-system-for-large-scale-graph-processing/) system, the program proceeds in discrete "super-steps." A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an `inactive` state. A node becomes `active` when it receives a new message (state) on any of its incoming edges (or "channels"). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to `halt` by marking themselves as `inactive`. The graph execution terminates when all nodes are `inactive` and no messages are in transit. ## StateGraph The `StateGraph` class is the main graph class to use. This is parameterized by a user defined `State` object. ## Compiling your graph To build your graph, you first define the [state](#state), you then add [nodes](#nodes) and [edges](#edges), and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like [checkpointers](https://langchain-ai.github.io/langgraph/concepts/persistence/). You compile your graph by just calling the `.compile` method: ```python graph = graph_builder.compile(...) ``` You **MUST** compile your graph before you can use it. ## State The first thing you do when you define a graph is define the `State` of the graph. The `State` consists of the [schema of the graph](#schema) as well as [`reducer` functions](#reducers) which specify how to apply updates to the state. The schema of the `State` will be the input schema to all `Nodes` and `Edges` in the graph, and can be either a `TypedDict` or a `Pydantic` model. All `Nodes` will emit updates to the `State` which are then applied using the specified `reducer` function. ### Schema The main documented way to specify the schema of a graph is by using `TypedDict`. However, we also support [using a Pydantic BaseModel](https://langchain-ai.github.io/langgraph/how-tos/graph-api/#use-pydantic-models-for-graph-state) as your graph state to add **default values** and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the [guide here](https://langchain-ai.github.io/langgraph/how-tos/graph-api/#define-input-and-output-schemas) for how to use. #### Multiple schemas Typically, all graph nodes communicate with a single schema. This means that they will read and write to the same state channels. But, there are cases where we want more control over this: * Internal nodes can pass information that is not required in the graph's input / output. * We may also want to use different input / output schemas for the graph. The output might, for example, only contain a single relevant output key. It is possible to have nodes write to private state channels inside the graph for internal node communication. We can simply define a private schema, `PrivateState`. See [this guide](https://langchain-ai.github.io/langgraph/how-tos/graph-api/#pass-private-state-between-nodes) for more detail. It is also possible to define explicit input and output schemas for a graph. In these cases, we define an "internal" schema that contains *all* keys relevant to graph operations. But, we also define `input` and `output` schemas that are sub-sets of the "internal" schema to constrain the input and output of the graph. See [this guide](https://langchain-ai.github.io/langgraph/how-tos/graph-api/#define-input-and-output-schemas) for more detail. Let's look at an example: ```python class InputState(TypedDict): user_input: str class OutputState(TypedDict): graph_output: str class OverallState(TypedDict): foo: str user_input: str graph_output: str class PrivateState(TypedDict): bar: str def node_1(state: InputState) -> OverallState: # Write to OverallState return {"foo": state["user_input"] + " name"} def node_2(state: OverallState) -> PrivateState: # Read from OverallState, write to PrivateState return {"bar": state["foo"] + " is"} def node_3(state: PrivateState) -> OutputState: # Read from PrivateState, write to OutputState return {"graph_output": state["bar"] + " Lance"} builder = StateGraph(OverallState,input_schema=InputState,output_schema=OutputState) builder.add_node("node_1", node_1) builder.add_node("node_2", node_2) builder.add_node("node_3", node_3) builder.add_edge(START, "node_1") builder.add_edge("node_1", "node_2") builder.add_edge("node_2", "node_3") builder.add_edge("node_3", END) graph = builder.compile() graph.invoke({"user_input":"My"}) {'graph_output': 'My name is Lance'} ``` There are two subtle and important points to note here: 1. We pass `state: InputState` as the input schema to `node_1`. But, we write out to `foo`, a channel in `OverallState`. How can we write out to a state channel that is not included in the input schema? This is because a node *can write to any state channel in the graph state.* The graph state is the union of the state channels defined at initialization, which includes `OverallState` and the filters `InputState` and `OutputState`. 2. We initialize the graph with `StateGraph(OverallState,input_schema=InputState,output_schema=OutputState)`. So, how can we write to `PrivateState` in `node_2`? How does the graph gain access to this schema if it was not passed in the `StateGraph` initialization? We can do this because *nodes can also declare additional state channels* as long as the state schema definition exists. In this case, the `PrivateState` schema is defined, so we can add `bar` as a new state channel in the graph and write to it. ### Reducers Reducers are key to understanding how updates from nodes are applied to the `State`. Each key in the `State` has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: #### Default Reducer These two examples show how to use the default reducer: **Example A:** ```python from typing_extensions import TypedDict class State(TypedDict): foo: int bar: list[str] ``` In this example, no reducer functions are specified for any key. Let's assume the input to the graph is `{"foo": 1, "bar": ["hi"]}`. Let's then assume the first `Node` returns `{"foo": 2}`. This is treated as an update to the state. Notice that the `Node` does not need to return the whole `State` schema - just an update. After applying this update, the `State` would then be `{"foo": 2, "bar": ["hi"]}`. If the second node returns `{"bar": ["bye"]}` then the `State` would then be `{"foo": 2, "bar": ["bye"]}` **Example B:** ```python from typing import Annotated from typing_extensions import TypedDict from operator import add class State(TypedDict): foo: int bar: Annotated[list[str], add] ``` In this example, we've used the `Annotated` type to specify a reducer function (`operator.add`) for the second key (`bar`). Note that the first key remains unchanged. Let's assume the input to the graph is `{"foo": 1, "bar": ["hi"]}`. Let's then assume the first `Node` returns `{"foo": 2}`. This is treated as an update to the state. Notice that the `Node` does not need to return the whole `State` schema - just an update. After applying this update, the `State` would then be `{"foo": 2, "bar": ["hi"]}`. If the second node returns `{"bar": ["bye"]}` then the `State` would then be `{"foo": 2, "bar": ["hi", "bye"]}`. Notice here that the `bar` key is updated by adding the two lists together. ### Working with Messages in Graph State #### Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's [`ChatModel`](https://python.langchain.com/docs/concepts/#chat-models) in particular accepts a list of `Message` objects as inputs. These messages come in a variety of forms such as `HumanMessage` (user input) or `AIMessage` (LLM response). To read more about what message objects are, please refer to [this](https://python.langchain.com/docs/concepts/#messages) conceptual guide. #### Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of `Message` objects and annotate it with a reducer function (see `messages` key in the example below). The reducer function is vital to telling the graph how to update the list of `Message` objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use `operator.add` as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use `operator.add`, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt `add_messages` function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. #### Serialization In addition to keeping track of message IDs, the `add_messages` function will also try to deserialize messages into LangChain `Message` objects whenever a state update is received on the `messages` channel. See more information on LangChain serialization/deserialization [here](https://python.langchain.com/docs/how_to/serialization/). This allows sending graph inputs / state updates in the following format: ```python # this is supported {"messages": [HumanMessage(content="message")]} # and this is also supported {"messages": [{"type": "human", "content": "message"}]} ``` Since the state updates are always deserialized into LangChain `Messages` when using `add_messages`, you should use dot notation to access message attributes, like `state["messages"][-1].content`. Below is an example of a graph that uses `add_messages` as its reducer function. ```python from langchain_core.messages import AnyMessage from langgraph.graph.message import add_messages from typing import Annotated from typing_extensions import TypedDict class GraphState(TypedDict): messages: Annotated[list[AnyMessage], add_messages] ``` #### MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called `MessagesState` which makes it easy to use messages. `MessagesState` is defined with a single `messages` key which is a list of `AnyMessage` objects and uses the `add_messages` reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: ```python from langgraph.graph import MessagesState class State(MessagesState): documents: list[str] ``` ## Nodes In LangGraph, nodes are typically python functions (sync or async) where the **first** positional argument is the [state](#state), and (optionally), the **second** positional argument is a "config", containing optional [configurable parameters](#configuration) (such as a `thread_id`). Similar to `NetworkX`, you add these nodes to a graph using the [`add_node`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph.add_node) method: ```python from typing_extensions import TypedDict from langchain_core.runnables import RunnableConfig from langgraph.graph import StateGraph class State(TypedDict): input: str results: str builder = StateGraph(State) def my_node(state: State, config: RunnableConfig): print("In node: ", config["configurable"]["user_id"]) return {"results": f"Hello, {state['input']}!"} # The second argument is optional def my_other_node(state: State): return state builder.add_node("my_node", my_node) builder.add_node("other_node", my_other_node) ... ``` Behind the scenes, functions are converted to [RunnableLambda](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.base.RunnableLambda.html#langchain_core.runnables.base.RunnableLambda)s, which add batch and async support to your function, along with native tracing and debugging. If you add a node to a graph without specifying a name, it will be given a default name equivalent to the function name. ```python builder.add_node(my_node) # You can then create edges to/from this node by referencing it as `"my_node"` ``` ### `START` Node The `START` Node is a special node that represents the node that sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. ```python from langgraph.graph import START graph.add_edge(START, "node_a") ``` ### `END` Node The `END` Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. ```python from langgraph.graph import END graph.add_edge("node_a", END) ``` ### Node Caching LangGraph supports caching of tasks/nodes based on the input to the node. To use caching: * Specify a cache when compiling a graph (or specifying an entrypoint) * Specify a cache policy for nodes. Each cache policy supports: * `key_func` used to generate a cache key based on the input to a node, which defaults to a `hash` of the input with pickle. * `ttl`, the time to live for the cache in seconds. If not specified, the cache will never expire. For example: ```python import time from typing_extensions import TypedDict from langgraph.graph import StateGraph from langgraph.cache.memory import InMemoryCache from langgraph.types import CachePolicy class State(TypedDict): x: int result: int builder = StateGraph(State) def expensive_node(state: State) -> dict[str, int]: # expensive computation time.sleep(2) return {"result": state["x"] * 2} builder.add_node("expensive_node", expensive_node, cache_policy=CachePolicy(ttl=3)) builder.set_entry_point("expensive_node") builder.set_finish_point("expensive_node") graph = builder.compile(cache=InMemoryCache()) print(graph.invoke({"x": 5}, stream_mode='updates')) #[{'expensive_node': {'result': 10}}] print(graph.invoke({"x": 5}, stream_mode='updates')) #[{'expensive_node': {'result': 10}, '__metadata__': {'cached': True}}] ``` ## Edges Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: * Normal Edges: Go directly from one node to the next. * Conditional Edges: Call a function to determine which node(s) to go to next. * Entry Point: Which node to call first when user input arrives. * Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives. A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, **all** of those destination nodes will be executed in parallel as a part of the next superstep. ### Normal Edges If you **always** want to go from node A to node B, you can use the [`add_edge`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph.add_edge) method directly. ```python graph.add_edge("node_a", "node_b") ``` ### Conditional Edges If you want to **optionally** route to 1 or more edges (or optionally terminate), you can use the [`add_conditional_edges`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph.add_conditional_edges) method. This method accepts the name of a node and a "routing function" to call after that node is executed: ```python graph.add_conditional_edges("node_a", routing_function) ``` Similar to nodes, the `routing_function` accepts the current `state` of the graph and returns a value. By default, the return value `routing_function` is used as the name of the node (or list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the `routing_function`'s output to the name of the next node. ```python graph.add_conditional_edges("node_a", routing_function, {True: "node_b", False: "node_c"}) ``` > Tip > > Use [`Command`](#command) instead of conditional edges if you want to combine state updates and routing in a single function. ### Entry Point The entry point is the first node(s) that are run when the graph starts. You can use the [`add_edge`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph.add_edge) method from the virtual [`START`](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START) node to the first node to execute to specify where to enter the graph. ```python from langgraph.graph import START graph.add_edge(START, "node_a") ``` ### Conditional Entry Point A conditional entry point lets you start at different nodes depending on custom logic. You can use [`add_conditional_edges`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph.add_conditional_edges) from the virtual [`START`](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START) node to accomplish this. ```python from langgraph.graph import START graph.add_conditional_edges(START, routing_function) ``` You can optionally provide a dictionary that maps the `routing_function`'s output to the name of the next node. ```python graph.add_conditional_edges(START, routing_function, {True: "node_b", False: "node_c"}) ``` ## `Send` By default, `Nodes` and `Edges` are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of `State` to exist at the same time. A common example of this is with [map-reduce](https://langchain-ai.github.io/langgraph/how-tos/map-reduce/) design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input `State` to the downstream `Node` should be different (one for each generated object). To support this design pattern, LangGraph supports returning [`Send`](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Send) objects from conditional edges. `Send` takes two arguments: first is the name of the node, and second is the state to pass to that node. ```python def continue_to_jokes(state: OverallState): return [Send("generate_joke", {"subject": s}) for s in state['subjects']] graph.add_conditional_edges("node_a", continue_to_jokes) ``` ## `Command` It can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a [`Command`](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Command) object from node functions: ```python def my_node(state: State) -> Command[Literal["my_other_node"]]: return Command( # state update update={"foo": "bar"}, # control flow goto="my_other_node" ) ``` With `Command` you can also achieve dynamic control flow behavior (identical to [conditional edges](#conditional-edges)): ```python def my_node(state: State) -> Command[Literal["my_other_node"]]: if state["foo"] == "bar": return Command(update={"foo": "baz"}, goto="my_other_node") ``` > Important > > When returning `Command` in your node functions, you must add return type annotations with the list of node names the node is routing to, e.g. `Command[Literal["my_other_node"]]`. This is necessary for the graph rendering and tells LangGraph that `my_node` can navigate to `my_other_node`. Check out this [how-to guide](https://langchain-ai.github.io/langgraph/how-tos/graph-api/#combine-control-flow-and-state-updates-with-command) for an end-to-end example of how to use `Command`. ### When should I use Command instead of conditional edges? Use `Command` when you need to **both** update the graph state **and** route to a different node. For example, when implementing [multi-agent handoffs](https://langchain-ai.github.io/langgraph/concepts/multi_agent/#handoffs) where it's important to route to a different agent and pass some information to that agent. Use [conditional edges](#conditional-edges) to route between nodes conditionally without updating the state. ### Navigating to a node in a parent graph If you are using [subgraphs](https://langchain-ai.github.io/langgraph/concepts/subgraphs/), you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify `graph=Command.PARENT` in `Command`: ```python def my_node(state: State) -> Command[Literal["other_subgraph"]]: return Command( update={"foo": "bar"}, goto="other_subgraph", # where `other_subgraph` is a node in the parent graph graph=Command.PARENT ) ``` > Note > > Setting `graph` to `Command.PARENT` will navigate to the closest parent graph. > State updates with `Command.PARENT` > > When you send updates from a subgraph node to a parent graph node for a key that's shared by both parent and subgraph [state schemas](#schema), you **must** define a [reducer](#reducers) for the key you're updating in the parent graph state. See this [example](https://langchain-ai.github.io/langgraph/how-tos/graph-api/#navigate-to-a-node-in-a-parent-graph). This is particularly useful when implementing [multi-agent handoffs](https://langchain-ai.github.io/langgraph/concepts/multi_agent/#handoffs). Check out [this guide](https://langchain-ai.github.io/langgraph/how-tos/graph-api/#navigate-to-a-node-in-a-parent-graph) for detail. ### Using inside tools A common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation. Refer to [this guide](https://langchain-ai.github.io/langgraph/how-tos/graph-api/#use-inside-tools) for detail. ### Human-in-the-loop `Command` is an important part of human-in-the-loop workflows: when using `interrupt()` to collect user input, `Command` is then used to supply the input and resume execution via `Command(resume="User input")`. Check out [this conceptual guide](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/) for more information. ## Graph Migrations LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. * For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc) * For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution. * For modifying state, we have full backwards and forwards compatibility for adding and removing keys * State keys that are renamed lose their saved state in existing threads * State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution. ## Configuration When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single "cognitive architecture" (the graph) but have multiple different instance of it. You can optionally specify a `config_schema` when creating a graph. ```python class ConfigSchema(TypedDict): llm: str graph = StateGraph(State, config_schema=ConfigSchema) ``` You can then pass this configuration into the graph using the `configurable` config field. ```python config = {"configurable": {"llm": "anthropic"}} graph.invoke(inputs, config=config) ``` You can then access and use this configuration inside a node or conditional edge: ```python def node_a(state, config): llm_type = config.get("configurable", {}).get("llm", "openai") llm = get_llm(llm_type) ... ``` See [this guide](https://langchain-ai.github.io/langgraph/how-tos/graph-api/#add-runtime-configuration) for a full breakdown on configuration. ### Recursion Limit The recursion limit sets the maximum number of [super-steps](#graphs) the graph can execute during a single execution. Once the limit is reached, LangGraph will raise `GraphRecursionError`. By default this value is set to 25 steps. The recursion limit can be set on any graph at runtime, and is passed to `.invoke`/.stream` via the config dictionary. Importantly, `recursion_limit` is a standalone `config` key and should not be passed inside the `configurable` key as all other user-defined configuration. See the example below: ```python graph.invoke(inputs, config={"recursion_limit": 5, "configurable":{"llm": "anthropic"}}) ``` Read [this how-to](https://langchain-ai.github.io/langgraph/how-tos/recursion-limit/) to learn more about how the recursion limit works. ## Visualization It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See [this how-to guide](https://langchain-ai.github.io/langgraph/how-tos/graph-api/#visualize-your-graph) for more info.

---

# URL: https://langchain-ai.github.io/langgraph/tutorials/get-started/2-add-tools
**Depth**: 1

# Add tools To handle queries that your chatbot can't answer "from memory", integrate a web search tool. The chatbot can use this tool to find relevant information and provide better responses. Note: This tutorial builds on [Build a basic chatbot](https://langchain-ai.github.io/langgraph/tutorials/get-started/1-build-basic-chatbot/). ## Prerequisites Before you start this tutorial, ensure you have the following: * An API key for the [Tavily Search Engine](https://python.langchain.com/docs/integrations/tools/tavily_search/). ## 1. Install the search engine Install the requirements to use the [Tavily Search Engine](https://python.langchain.com/docs/integrations/tools/tavily_search/): ``` pip install -U langchain-tavily ``` ## 2. Configure your environment Configure your environment with your search engine API key: ```python import os import getpass def _set_env(var: str): if not os.environ.get(var): os.environ[var] = getpass.getpass(f"{var}: ") _set_env("TAVILY_API_KEY") ``` ``` os.environ["TAVILY_API_KEY"]: "········" ``` ## 3. Define the tool Define the web search tool: API Reference: [TavilySearch](https://python.langchain.com/api_reference/tavily/tavily_search/langchain_tavily.tavily_search.TavilySearch.html) ```python from langchain_tavily import TavilySearch tool = TavilySearch(max_results=2) tools = [tool] tool.invoke("What's a 'node' in LangGraph?") ``` The results are page summaries our chat bot can use to answer questions: ``` {'query': "What's a 'node' in LangGraph?", 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': "Introduction to LangGraph: A Beginner's Guide - Medium", 'url': 'https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141', 'content': 'Stateful Graph: LangGraph revolves around the concept of a stateful graph, where each node in the graph represents a step in your computation, and the graph maintains a state that is passed around and updated as the computation progresses. LangGraph supports conditional edges, allowing you to dynamically determine the next node to execute based on the current state of the graph. We define nodes for classifying the input, handling greetings, and handling search queries. def classify_input_node(state): LangGraph is a versatile tool for building complex, stateful applications with LLMs. By understanding its core concepts and working through simple examples, beginners can start to leverage its power for their projects. Remember to pay attention to state management, conditional edges, and ensuring there are no dead-end nodes in your graph.', 'score': 0.7065353, 'raw_content': None}, {'title': 'LangGraph Tutorial: What Is LangGraph and How to Use It?', 'url': 'https://www.datacamp.com/tutorial/langgraph-tutorial', 'content': 'LangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner. By managing the flow of data and the sequence of operations, LangGraph allows developers to focus on the high-level logic of their applications rather than the intricacies of agent coordination. Whether you need a chatbot that can handle various types of user requests or a multi-agent system that performs complex tasks, LangGraph provides the tools to build exactly what you need. LangGraph significantly simplifies the development of complex LLM applications by providing a structured framework for managing state and coordinating agent interactions.', 'score': 0.5008063, 'raw_content': None}], 'response_time': 1.38} ``` ## 4. Define the graph For the `StateGraph` you created in the [first tutorial](https://langchain-ai.github.io/langgraph/tutorials/get-started/1-build-basic-chatbot/), add `bind_tools` on the LLM. This lets the LLM know the correct JSON format to use if it wants to use the search engine. Let's first select our LLM: ### OpenAI ``` pip install -U "langchain[openai]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["OPENAI_API_KEY"] = "sk-..." llm = init_chat_model("openai:gpt-4.1") ``` 👉 Read the [OpenAI integration docs](https://python.langchain.com/docs/integrations/chat/openai/) ### Anthropic ``` pip install -U "langchain[anthropic]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["ANTHROPIC_API_KEY"] = "sk-..." llm = init_chat_model("anthropic:claude-3-5-sonnet-latest") ``` 👉 Read the [Anthropic integration docs](https://python.langchain.com/docs/integrations/chat/anthropic/) ### Azure ``` pip install -U "langchain[openai]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["AZURE_OPENAI_API_KEY"] = "..." os.environ["AZURE_OPENAI_ENDPOINT"] = "..." os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview" llm = init_chat_model( "azure_openai:gpt-4.1", azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"], ) ``` 👉 Read the [Azure integration docs](https://python.langchain.com/docs/integrations/chat/azure_chat_openai/) ### Google Gemini ``` pip install -U "langchain[google-genai]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["GOOGLE_API_KEY"] = "..." llm = init_chat_model("google_genai:gemini-2.0-flash") ``` 👉 Read the [Google GenAI integration docs](https://python.langchain.com/docs/integrations/chat/google_generative_ai/) ### AWS Bedrock ``` pip install -U "langchain[aws]" ``` ```python from langchain.chat_models import init_chat_model # Follow the steps here to configure your credentials: # https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html llm = init_chat_model( "anthropic.claude-3-5-sonnet-20240620-v1:0", model_provider="bedrock_converse", ) ``` 👉 Read the [AWS Bedrock integration docs](https://python.langchain.com/docs/integrations/chat/bedrock/) We can now incorporate it into a `StateGraph`: ```python from typing import Annotated from typing_extensions import TypedDict from langgraph.graph import StateGraph, START, END from langgraph.graph.message import add_messages class State(TypedDict): messages: Annotated[list, add_messages] graph_builder = StateGraph(State) # Modification: tell the LLM which tools it can call # highlight-next-line llm_with_tools = llm.bind_tools(tools) def chatbot(state: State): return {"messages": [llm_with_tools.invoke(state["messages"])]} graph_builder.add_node("chatbot", chatbot) ``` ## 5. Create a function to run the tools Now, create a function to run the tools if they are called. Do this by adding the tools to a new node called`BasicToolNode` that checks the most recent message in the state and calls tools if the message contains `tool_calls`. It relies on the LLM's `tool_calling` support, which is available in Anthropic, OpenAI, Google Gemini, and a number of other LLM providers. API Reference: [ToolMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolMessage.html) ```python import json from langchain_core.messages import ToolMessage class BasicToolNode: """A node that runs the tools requested in the last AIMessage.""" def __init__(self, tools: list) -> None: self.tools_by_name = {tool.name: tool for tool in tools} def __call__(self, inputs: dict): if messages := inputs.get("messages", []): message = messages[-1] else: raise ValueError("No message found in input") outputs = [] for tool_call in message.tool_calls: tool_result = self.tools_by_name[tool_call["name"]].invoke( tool_call["args"] ) outputs.append( ToolMessage( content=json.dumps(tool_result), name=tool_call["name"], tool_call_id=tool_call["id"], ) ) return {"messages": outputs} tool_node = BasicToolNode(tools=[tool]) graph_builder.add_node("tools", tool_node) ``` Note: If you do not want to build this yourself in the future, you can use LangGraph's prebuilt [ToolNode](https://langchain-ai.github.io/langgraph/reference/agents/#langgraph.prebuilt.tool_node.ToolNode). ## 6. Define the `conditional_edges` With the tool node added, now you can define the `conditional_edges`. **Edges** route the control flow from one node to the next. **Conditional edges** start from a single node and usually contain "if" statements to route to different nodes depending on the current graph state. These functions receive the current graph `state` and return a string or list of strings indicating which node(s) to call next. Next, define a router function called `route_tools` that checks for `tool_calls` in the chatbot's output. Provide this function to the graph by calling `add_conditional_edges`, which tells the graph that whenever the `chatbot` node completes to check this function to see where to go next. The condition will route to `tools` if tool calls are present and `END` if not. Because the condition can return `END`, you do not need to explicitly set a `finish_point` this time. ```python def route_tools( state: State, ): """ Use in the conditional_edge to route to the ToolNode if the last message has tool calls. Otherwise, route to the end. """ if isinstance(state, list): ai_message = state[-1] elif messages := state.get("messages", []): ai_message = messages[-1] else: raise ValueError(f"No messages found in input state to tool_edge: {state}") if hasattr(ai_message, "tool_calls") and len(ai_message.tool_calls) > 0: return "tools" return END # The `tools_condition` function returns "tools" if the chatbot asks to use a tool, and "END" if # it is fine directly responding. This conditional routing defines the main agent loop. graph_builder.add_conditional_edges( "chatbot", route_tools, # The following dictionary lets you tell the graph to interpret the condition's outputs as a specific node # It defaults to the identity function, but if you # want to use a node named something else apart from "tools", # You can update the value of the dictionary to something else # e.g., "tools": "my_tools" {"tools": "tools", END: END}, ) # Any time a tool is called, we return to the chatbot to decide the next step graph_builder.add_edge("tools", "chatbot") graph_builder.add_edge(START, "chatbot") graph = graph_builder.compile() ``` Note: You can replace this with the prebuilt [tools\\_condition](https://langchain-ai.github.io/langgraph/reference/prebuilt/#tools_condition) to be more concise. ## 7. Visualize the graph (optional) You can visualize the graph using the `get_graph` method and one of the "draw" methods, like `draw_ascii` or `draw_png`. The `draw` methods each require additional dependencies. ```python from IPython.display import Image, display try: display(Image(graph.get_graph().draw_mermaid_png())) except Exception: # This requires some extra dependencies and is optional pass ``` ## 8. Ask the bot questions Now you can ask the chatbot questions outside its training data: ```python def stream_graph_updates(user_input: str): for event in graph.stream({"messages": [{"role": "user", "content": user_input}]}): for value in event.values(): print("Assistant:", value["messages"][-1].content) while True: try: user_input = input("User: ") if user_input.lower() in ["quit", "exit", "q"]: print("Goodbye!") break stream_graph_updates(user_input) except: # fallback if input() is not available user_input = "What do you know about LangGraph?" print("User: " + user_input) stream_graph_updates(user_input) break ``` ``` Assistant: [{'text': "To provide you with accurate and up-to-date information about LangGraph, I'll need to search for the latest details. Let me do that for you.", 'type': 'text'}, {'id': 'toolu_01Q588CszHaSvvP2MxRq9zRD', 'input': {'query': 'LangGraph AI tool information'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}] Assistant: [{'url': "https://www.langchain.com/langgraph", 'content': "LangGraph sets the foundation for how we can build and scale AI workloads \u2014 from conversational agents, complex task automation, to custom LLM-backed experiences that 'just work'. The next chapter in building complex production-ready features with LLMs is agentic, and with LangGraph and LangSmith, LangChain delivers an out-of-the-box solution ..."}, {'url': "https://github.com/langchain-ai/langgraph", 'content': "Overview. LangGraph is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Compared to other LLM frameworks, it offers these core benefits: cycles, controllability, and persistence. LangGraph allows you to define flows that involve cycles, essential for most agentic architectures ..."}] Assistant: Based on the search results, I can provide you with information about LangGraph: 1. Purpose: LangGraph is a library designed for building stateful, multi-actor applications with Large Language Models (LLMs). It's particularly useful for creating agent and multi-agent workflows. 2. Developer: LangGraph is developed by LangChain, a company known for its tools and frameworks in the AI and LLM space. 3. Key Features: * Cycles: LangGraph allows the definition of flows that involve cycles, which is essential for most agentic architectures. * Controllability: It offers enhanced control over the application flow. * Persistence: The library provides ways to maintain state and persistence in LLM-based applications. 4. Use Cases: LangGraph can be used for various applications, including: * Conversational agents * Complex task automation * Custom LLM-backed experiences 5. Integration: LangGraph works in conjunction with LangSmith, another tool by LangChain, to provide an out-of-the-box solution for building complex, production-ready features with LLMs. 6. Significance: ... LangGraph is noted to offer unique benefits compared to other LLM frameworks, particularly in its ability to handle cycles, provide controllability, and maintain persistence. LangGraph appears to be a significant tool in the evolving landscape of LLM-based application development, offering developers new ways to create more complex, stateful, and interactive AI systems. Goodbye! Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings... ``` ## 9. Use prebuilts For ease of use, adjust your code to replace the following with LangGraph prebuilt components. These have built in functionality like parallel API execution. * `BasicToolNode` is replaced with the prebuilt [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#toolnode) * `route_tools` is replaced with the prebuilt [tools\\_condition](https://langchain-ai.github.io/langgraph/reference/prebuilt/#tools_condition) ### OpenAI ``` pip install -U "langchain[openai]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["OPENAI_API_KEY"] = "sk-..." llm = init_chat_model("openai:gpt-4.1") ``` 👉 Read the [OpenAI integration docs](https://python.langchain.com/docs/integrations/chat/openai/) ### Anthropic ``` pip install -U "langchain[anthropic]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["ANTHROPIC_API_KEY"] = "sk-..." llm = init_chat_model("anthropic:claude-3-5-sonnet-latest") ``` 👉 Read the [Anthropic integration docs](https://python.langchain.com/docs/integrations/chat/anthropic/) ### Azure ``` pip install -U "langchain[openai]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["AZURE_OPENAI_API_KEY"] = "..." os.environ["AZURE_OPENAI_ENDPOINT"] = "..." os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview" llm = init_chat_model( "azure_openai:gpt-4.1", azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"], ) ``` 👉 Read the [Azure integration docs](https://python.langchain.com/docs/integrations/chat/azure_chat_openai/) ### Google Gemini ``` pip install -U "langchain[google-genai]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["GOOGLE_API_KEY"] = "..." llm = init_chat_model("google_genai:gemini-2.0-flash") ``` 👉 Read the [Google GenAI integration docs](https://python.langchain.com/docs/integrations/chat/google_generative_ai/) ### AWS Bedrock ``` pip install -U "langchain[aws]" ``` ```python from langchain.chat_models import init_chat_model # Follow the steps here to configure your credentials: # https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html llm = init_chat_model( "anthropic.claude-3-5-sonnet-20240620-v1:0", model_provider="bedrock_converse", ) ``` 👉 Read the [AWS Bedrock integration docs](https://python.langchain.com/docs/integrations/chat/bedrock/) ```python from typing import Annotated from langchain_tavily import TavilySearch from langchain_core.messages import BaseMessage from typing_extensions import TypedDict from langgraph.graph import StateGraph, START, END from langgraph.graph.message import add_messages from langgraph.prebuilt import ToolNode, tools_condition class State(TypedDict): messages: Annotated[list, add_messages] graph_builder = StateGraph(State) tool = TavilySearch(max_results=2) tools = [tool] llm_with_tools = llm.bind_tools(tools) def chatbot(state: State): return {"messages": [llm_with_tools.invoke(state["messages"])]} graph_builder.add_node("chatbot", chatbot) tool_node = ToolNode(tools=[tool]) graph_builder.add_node("tools", tool_node) graph_builder.add_conditional_edges( "chatbot", tools_condition, ) # Any time a tool is called, we return to the chatbot to decide the next step graph_builder.add_edge("tools", "chatbot") graph_builder.add_edge(START, "chatbot") graph = graph_builder.compile() ``` **Congratulations!** You've created a conversational agent in LangGraph that can use a search engine to retrieve updated information when needed. Now it can handle a wider range of user queries. To inspect all the steps your agent just took, check out this [LangSmith trace](https://smith.langchain.com/public/4fbd7636-25af-4638-9587-5a02fdbb0172/r). ## Next steps The chatbot cannot remember past interactions on its own, which limits its ability to have coherent, multi-turn conversations. In the next part, you will [add **memory**](https://langchain-ai.github.io/langgraph/tutorials/get-started/3-add-memory/) to address this.

---

# URL: https://langchain-ai.github.io/langgraph/agents/overview
**Depth**: 1

# Agent development using prebuilt components LangGraph provides both low-level primitives and high-level prebuilt components for building agent-based applications. This section focuses on the prebuilt, ready-to-use components designed to help you construct agentic systems quickly and reliably—without the need to implement orchestration, memory, or human feedback handling from scratch. ## What is an agent? An *agent* consists of three components: a **large language model (LLM)**, a set of **tools** it can use, and a **prompt** that provides instructions. The LLM operates in a loop. In each iteration, it selects a tool to invoke, provides input, receives the result (an observation), and uses that observation to inform the next action. The loop continues until a stopping condition is met — typically when the agent has gathered enough information to respond to the user. ![Agent loop: the LLM selects tools and uses their outputs to fulfill a user request.](https://langchain-ai.github.io/langgraph/agents/assets/agent.png) ## Key features LangGraph includes several capabilities essential for building robust, production-ready agentic systems: * [**Memory integration**](https://langchain-ai.github.io/langgraph/how-tos/memory/add-memory/): Native support for *short-term* (session-based) and *long-term* (persistent across sessions) memory, enabling stateful behaviors in chatbots and assistants. * [**Human-in-the-loop control**](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/): Execution can pause *indefinitely* to await human feedback—unlike websocket-based solutions limited to real-time interaction. This enables asynchronous approval, correction, or intervention at any point in the workflow. * [**Streaming support**](https://langchain-ai.github.io/langgraph/how-tos/streaming/): Real-time streaming of agent state, model tokens, tool outputs, or combined streams. * [**Deployment tooling**](https://langchain-ai.github.io/langgraph/tutorials/langgraph-platform/local-server/): Includes infrastructure-free deployment tools. [**LangGraph Platform**](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/) supports testing, debugging, and deployment. * **[Studio](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/)**: A visual IDE for inspecting and debugging workflows. * Supports multiple [**deployment options**](https://langchain-ai.github.io/langgraph/concepts/deployment_options.md) for production. ## High-level building blocks LangGraph comes with a set of prebuilt components that implement common agent behaviors and workflows. These abstractions are built on top of the LangGraph framework, offering a faster path to production while remaining flexible for advanced customization. Using LangGraph for agent development allows you to focus on your application's logic and behavior, instead of building and maintaining the supporting infrastructure for state, memory, and human feedback. ## Package ecosystem The high-level components are organized into several packages, each with a specific focus. | Package | Description be able to reproduce the code and run it on my machine. ## Visualize an agent graph Use the following tool to visualize the graph generated by `create_react_agent` and to view an outline of the corresponding code. It allows you to explore the infrastructure of the agent as defined by the presence of: * `tools`: A list of tools (functions, APIs, or other callable objects) that the agent can use to perform tasks. * `pre_model_hook`: A function that is called before the model is invoked. It can be used to condense messages or perform other preprocessing tasks. * `post_model_hook`: A function that is called after the model is invoked. It can be used to implement guardrails, human-in-the-loop flows, or other postprocessing tasks. * `response_format`: A data structure used to constrain the type of the final output, e.g., a `pydantic` `BaseModel`. ### Features `tools` `pre_model_hook` `post_model_hook` `response_format` ### Graph ![graph image](../assets/react_agent_graphs/0001.svg) The following code snippet shows how to create the above agent (and underlying graph) with `create_react_agent`: ```python from langgraph.prebuilt import create_react_agent from langchain_openai import ChatOpenAI model = ChatOpenAI("o4-mini") def tool() -> None: """Testing tool.""" ... agent = create_react_agent( model, tools=[tool], ) agent.get_graph().draw_mermaid_png() ```

---

# URL: https://langchain-ai.github.io/langgraph/agents/prebuilt
**Depth**: 1

# Community Agents If you’re looking for other prebuilt libraries, explore the community-built options below. These libraries can extend LangGraph's functionality in various ways. ## 📚 Available Libraries | Name | GitHub URL | Description | Weekly Downloads | Stars | | -------------------------- | -------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------- | ---------------- | ----------------------------------------------------------------------------------------------------------------------------------- | | **langchain-mcp-adapters** | [langchain-ai/langchain-mcp-adapters](https://github.com/langchain-ai/langchain-mcp-adapters) | Make Anthropic Model Context Protocol (MCP) tools compatible with LangGraph agents. | 82969 | ![GitHub stars](https://img.shields.io/github/stars/langchain-ai/langchain-mcp-adapters?style=social) | | **langgraph-supervisor** | [langchain-ai/langgraph-supervisor-py](https://github.com/langchain-ai/langgraph-supervisor-py) | Build supervisor multi-agent systems with LangGraph. | 43296 | ![GitHub stars](https://img.shields.io/github/stars/langchain-ai/langgraph-supervisor-py?style=social) | | **trustcall** | [hinthornw/trustcall](https://github.com/hinthornw/trustcall) | Tenacious tool calling built on LangGraph. | 34813 | ![GitHub stars](https://img.shields.io/github/stars/hinthornw/trustcall?style=social) | | **langmem** | [langchain-ai/langmem](https://github.com/langchain-ai/langmem) | Build agents that learn and adapt from interactions over time. | 16768 | ![GitHub stars](https://img.shields.io/github/stars/langchain-ai/langmem?style=social) | | **langgraph-swarm** | [langchain-ai/langgraph-swarm-py](https://github.com/langchain-ai/langgraph-swarm-py) | Build swarm-style multi-agent systems using LangGraph. | 6188 | ![GitHub stars](https://img.shields.io/github/stars/langchain-ai/langgraph-swarm-py?style=social) | | **open-deep-research** | [langchain-ai/open_deep_research](https://github.com/langchain-ai/open_deep_research) | Open source assistant for iterative web research and report writing. | 1098 | ![GitHub stars](https://img.shields.io/github/stars/langchain-ai/open_deep_research?style=social) | | **langgraph-codeact** | [langchain-ai/langgraph-codeact](https://github.com/langchain-ai/langgraph-codeact) | LangGraph implementation of CodeAct agent that generates and executes code instead of tool calling. | 553 | ![GitHub stars](https://img.shields.io/github/stars/langchain-ai/langgraph-codeact?style=social) | | **langgraph-bigtool** | [langchain-ai/langgraph-bigtool](https://github.com/langchain-ai/langgraph-bigtool) | Build LangGraph agents with large numbers of tools. | 372 | ![GitHub stars](https://img.shields.io/github/stars/langchain-ai/langgraph-bigtool?style=social) | | **langgraph-reflection** | [langchain-ai/langgraph-reflection](https://github.com/langchain-ai/langgraph-reflection) | LangGraph agent that runs a reflection step. | 370 | ![GitHub stars](https://img.shields.io/github/stars/langchain-ai/langgraph-reflection?style=social) | | **ai-data-science-team** | [business-science/ai-data-science-team](https://github.com/business-science/ai-data-science-team) | An AI-powered data science team of agents to help you perform common data science tasks 10X faster. | 271 | ![GitHub stars](https://img.shields.io/github/stars/business-science/ai-data-science-team?style=social) | | **nodeology** | [xyin-anl/Nodeology](https://github.com/xyin-anl/Nodeology) | Enable researcher to build scientific workflows easily with simplified interface. | 30 | ![GitHub stars](https://img.shields.io/github/stars/xyin-anl/Nodeology?style=social) | | **breeze-agent** | [andrestorres123/breeze-agent](https://github.com/andrestorres123/breeze-agent) | A streamlined research system built inspired on STORM and built on LangGraph. | 24 | ![GitHub stars](https://img.shields.io/github/stars/andrestorres123/breeze-agent?style=social) | | **delve-taxonomy-generator** | [andrestorres123/delve](https://github.com/andrestorres123/delve) | A taxonomy generator for unstructured data | 18 | ![GitHub stars](https://img.shields.io/github/stars/andrestorres123/delve?style=social) | ## ✨ Contributing Your Library Have you built an awesome open-source library using LangGraph? We'd love to feature your project on the official LangGraph documentation pages! 🏆 To share your project, simply open a Pull Request adding an entry for your package in our [packages.yml](https://github.com/langchain-ai/langgraph/blob/main/docs/_scripts/third_party_page/packages.yml) file. **Guidelines** * Your repo must be distributed as an installable package (e.g., PyPI for Python, npm for JavaScript/TypeScript, etc.) 📦 * The repo should either use the Graph API (exposing a `StateGraph` instance) or the Functional API (exposing an `entrypoint`). * The package must include documentation (e.g., a `README.md` or docs site) explaining how to use it. We'll review your contribution and merge it in! Thanks for contributing! 🚀

---

# URL: https://langchain-ai.github.io/langgraph/tutorials/langgraph-platform/local-server
**Depth**: 1

# Run a local server This guide shows you how to run a LangGraph application locally. ## Prerequisites Before you begin, ensure you have the following: * An API key for [LangSmith](https://smith.langchain.com/settings) - free to sign up ## 1. Install the LangGraph CLI #### Python server ``` # Python >= 3.11 is required. pip install --upgrade "langgraph-cli[inmem]" ``` #### Node server ``` npx @langchain/langgraph-cli ``` ## 2. Create a LangGraph app 🌱 Create a new app from the [`new-langgraph-project-python` template](https://github.com/langchain-ai/new-langgraph-project) or [`new-langgraph-project-js` template](https://github.com/langchain-ai/new-langgraphjs-project). This template demonstrates a single-node application you can extend with your own logic. #### Python server ``` langgraph new path/to/your/app --template new-langgraph-project-python ``` #### Node server ``` langgraph new path/to/your/app --template new-langgraph-project-js ``` Additional templates If you use `langgraph new` without specifying a template, you will be presented with an interactive menu that will allow you to choose from a list of available templates. ## 3. Install dependencies In the root of your new LangGraph app, install the dependencies in `edit` mode so your local changes are used by the server: #### Python server ``` cd path/to/your/app pip install -e . ``` #### Node server ``` cd path/to/your/app yarn install ``` ## 4. Create a `.env` file You will find a `.env.example` in the root of your new LangGraph app. Create a `.env` file in the root of your new LangGraph app and copy the contents of the `.env.example` file into it, filling in the necessary API keys: ``` LANGSMITH_API_KEY=lsv2... ``` ## 5. Launch LangGraph Server 🚀 Start the LangGraph API server locally: #### Python server ``` langgraph dev ``` #### Node server ``` npx @langchain/langgraph-cli dev ``` Sample output: ``` > Ready! > > - API: http://localhost:2024 > > - Docs: http://localhost:2024/docs > > - LangGraph Studio Web UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024 ``` The `langgraph dev` command starts LangGraph Server in an in-memory mode. This mode is suitable for development and testing purposes. For production use, deploy LangGraph Server with access to a persistent storage backend. For more information, see [Deployment options](https://langchain-ai.github.io/langgraph/concepts/deployment_options/). ## 6. Test your application in LangGraph Studio [LangGraph Studio](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/) is a specialized UI that you can connect to LangGraph API server to visualize, interact with, and debug your application locally. Test your graph in LangGraph Studio by visiting the URL provided in the output of the `langgraph dev` command: ``` > - LangGraph Studio Web UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024 ``` For a LangGraph Server running on a custom host/port, update the baseURL parameter.  Safari compatibility Use the `--tunnel` flag with your command to create a secure tunnel, as Safari has limitations when connecting to localhost servers: ``` langgraph dev --tunnel ```  ## 7. Test the API #### Python SDK (async) 1. Install the LangGraph Python SDK: ``` pip install langgraph-sdk ``` 2. Send a message to the assistant (threadless run): ```python from langgraph_sdk import get_client import asyncio client = get_client(url="http://localhost:2024") async def main(): async for chunk in client.runs.stream( None, # Threadless run "agent", # Name of assistant. Defined in langgraph.json. input={ "messages": [{ "role": "human", "content": "What is LangGraph?", }], }, ): print(f"Receiving new event of type: {chunk.event}...") print(chunk.data) print("\n\n") asyncio.run(main()) ``` #### Python SDK (sync) 1. Install the LangGraph Python SDK: ``` pip install langgraph-sdk ``` 2. Send a message to the assistant (threadless run): ```python from langgraph_sdk import get_sync_client client = get_sync_client(url="http://localhost:2024") for chunk in client.runs.stream( None, # Threadless run "agent", # Name of assistant. Defined in langgraph.json. input={ "messages": [{ "role": "human", "content": "What is LangGraph?", }], }, stream_mode="messages-tuple", ): print(f"Receiving new event of type: {chunk.event}...") print(chunk.data) print("\n\n") ``` #### Javascript SDK 1. Install the LangGraph JS SDK: ``` npm install @langchain/langgraph-sdk ``` 2. Send a message to the assistant (threadless run): ```javascript const {Client} = await import("@langchain/langgraph-sdk"); // only set the apiUrl if you changed the default port when calling langgraph dev const client = new Client({apiUrl: "http://localhost:2024"}); const streamResponse = client.runs.stream( null, // Threadless run "agent", // Assistant ID { input: { "messages": [ {"role": "user", "content": "What is LangGraph?"} ] }, streamMode: "messages-tuple", } ); for await (const chunk of streamResponse) { console.log(`Receiving new event of type: ${chunk.event}...`); console.log(JSON.stringify(chunk.data)); console.log("\n\n"); } ``` #### Rest API ``` curl -s --request POST \ --url "http://localhost:2024/runs/stream" \ --header 'Content-Type: application/json' \ --data '{ "assistant_id": "agent", "input": { "messages": [ { "role": "human", "content": "What is LangGraph?" } ] }, "stream_mode": "messages-tuple" }' ``` ## Next steps Now that you have a LangGraph app running locally, take your journey further by exploring deployment and advanced features: * [Deployment quickstart](https://langchain-ai.github.io/langgraph/cloud/quick_start/): Deploy your LangGraph app using LangGraph Platform. * [LangGraph Platform overview](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/): Learn about foundational LangGraph Platform concepts. * [LangGraph Server API Reference](https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref.html): Explore the LangGraph Server API documentation. * [Python SDK Reference](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/): Explore the Python SDK API Reference. * [JS/TS SDK Reference](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/js_ts_sdk_ref/): Explore the JS/TS SDK API Reference.

---

# URL: https://langchain-ai.github.io/langgraph/tutorials/get-started/6-time-travel
**Depth**: 1

# Time travel In a typical chatbot workflow, the user interacts with the bot one or more times to accomplish a task. [Memory](https://langchain-ai.github.io/langgraph/tutorials/get-started/3-add-memory/) and a [human-in-the-loop](https://langchain-ai.github.io/langgraph/tutorials/get-started/4-human-in-the-loop/) enable checkpoints in the graph state and control future responses. What if you want a user to be able to start from a previous response and explore a different outcome? Or what if you want users to be able to rewind your chatbot's work to fix mistakes or try a different strategy, something that is common in applications like autonomous software engineers? You can create these types of experiences using LangGraph's built-in **time travel** functionality. Note: This tutorial builds on [Customize state](https://langchain-ai.github.io/langgraph/tutorials/get-started/5-customize-state/). ## 1. Rewind your graph Rewind your graph by fetching a checkpoint using the graph's `get_state_history` method. You can then resume execution at this previous point in time. 
[OpenAI](https://langchain-ai.github.io/langgraph/tutorials/get-started/6-time-travel/#__tabbed_1_1)[Anthropic](https://langchain-ai.github.io/langgraph/tutorials/get-started/6-time-travel/#__tabbed_1_2)[Azure](https://langchain-ai.github.io/langgraph/tutorials/get-started/6-time-travel/#__tabbed_1_3)[Google Gemini](https://langchain-ai.github.io/langgraph/tutorials/get-started/6-time-travel/#__tabbed_1_4)[AWS Bedrock](https://langchain-ai.github.io/langgraph/tutorials/get-started/6-time-travel/#__tabbed_1_5)
```
pip install -U "langchain[openai]"

```

```
import os
from langchain.chat_models import init_chat_model

os.environ["OPENAI_API_KEY"] = "sk-..."

llm = init_chat_model("openai:gpt-4.1")

```

👉 Read the [OpenAI integration docs](https://python.langchain.com/docs/integrations/chat/openai/)
```
pip install -U "langchain[anthropic]"

```

```
import os
from langchain.chat_models import init_chat_model

os.environ["ANTHROPIC_API_KEY"] = "sk-..."

llm = init_chat_model("anthropic:claude-3-5-sonnet-latest")

```

👉 Read the [Anthropic integration docs](https://python.langchain.com/docs/integrations/chat/anthropic/)
```
pip install -U "langchain[openai]"

```

```
import os
from langchain.chat_models import init_chat_model

os.environ["AZURE_OPENAI_API_KEY"] = "..."
os.environ["AZURE_OPENAI_ENDPOINT"] = "..."
os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview"

llm = init_chat_model(
    "azure_openai:gpt-4.1",
    azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"],
)

```

👉 Read the [Azure integration docs](https://python.langchain.com/docs/integrations/chat/azure_chat_openai/)
```
pip install -U "langchain[google-genai]"

```

```
import os
from langchain.chat_models import init_chat_model

os.environ["GOOGLE_API_KEY"] = "..."

llm = init_chat_model("google_genai:gemini-2.0-flash")

```

👉 Read the [Google GenAI integration docs](https://python.langchain.com/docs/integrations/chat/google_generative_ai/)
```
pip install -U "langchain[aws]"

```

```
from langchain.chat_models import init_chat_model

# Follow the steps here to configure your credentials:
# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html

llm = init_chat_model(
    "anthropic.claude-3-5-sonnet-20240620-v1:0",
    model_provider="bedrock_converse",
)

```

👉 Read the [AWS Bedrock integration docs](https://python.langchain.com/docs/integrations/chat/bedrock/)
_API Reference: [TavilySearch](https://python.langchain.com/api_reference/tavily/tavily_search/langchain_tavily.tavily_search.TavilySearch.html) | [BaseMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.base.BaseMessage.html) | [MemorySaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) | [START](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START) | [END](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.END) | [add_messages](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages) | [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode) | [tools_condition](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.tools_condition)_ ```python from typing import Annotated from langchain_tavily import TavilySearch from langchain_core.messages import BaseMessage from typing_extensions import TypedDict from langgraph.checkpoint.memory import MemorySaver from langgraph.graph import StateGraph, START, END from langgraph.graph.message import add_messages from langgraph.prebuilt import ToolNode, tools_condition class State(TypedDict): messages: Annotated[list, add_messages] graph_builder = StateGraph(State) tool = TavilySearch(max_results=2) tools = [tool] llm_with_tools = llm.bind_tools(tools) def chatbot(state: State): return {"messages": [llm_with_tools.invoke(state["messages"])]} graph_builder.add_node("chatbot", chatbot) tool_node = ToolNode(tools=[tool]) graph_builder.add_node("tools", tool_node) graph_builder.add_conditional_edges( "chatbot", tools_condition, ) graph_builder.add_edge("tools", "chatbot") graph_builder.add_edge(START, "chatbot") memory = MemorySaver() graph = graph_builder.compile(checkpointer=memory) ``` ## 2. Add steps Add steps to your graph. Every step will be checkpointed in its state history: ```python config = {"configurable": {"thread_id": "1"}} events = graph.stream( { "messages": [ { "role": "user", "content": ( "I'm learning LangGraph. " "Could you do some research on it for me?" ), }, ], }, config, stream_mode="values", ) for event in events: if "messages" in event: event["messages"][-1].pretty_print() ``` ``` ================================ Human Message ================================= I'm learning LangGraph. Could you do some research on it for me? ================================== Ai Message ================================== [{'text': "Certainly! I'd be happy to research LangGraph for you. To get the most up-to-date and accurate information, I'll use the Tavily search engine to look this up. Let me do that for you now.", 'type': 'text'}, {'id': 'toolu_01BscbfJJB9EWJFqGrN6E54e', 'input': {'query': 'LangGraph latest information and features'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}] Tool Calls: tavily_search_results_json (toolu_01BscbfJJB9EWJFqGrN6E54e) Call ID: toolu_01BscbfJJB9EWJFqGrN6E54e Args: query: LangGraph latest information and features ================================= Tool Message ================================= Name: tavily_search_results_json [{'url': 'https://blockchain.news/news/langchain-new-features-upcoming-events-update', 'content': "LangChain, a leading platform in the AI development space, has released its latest updates, showcasing new use cases and enhancements across its ecosystem. According to the LangChain Blog, the updates cover advancements in LangGraph Platform, LangSmith's self-improving evaluators, and revamped documentation for LangGraph."}, {'url': 'https://blog.langchain.dev/langgraph-platform-announce/', 'content': "With these learnings under our belt, we decided to couple some of our latest offerings under LangGraph Platform. LangGraph Platform today includes LangGraph Server, LangGraph Studio, plus the CLI and SDK. ... we added features in LangGraph Server to deliver on a few key value areas. Below, we'll focus on these aspects of LangGraph Platform."}] ================================== Ai Message ================================== Thank you for your patience. I've found some recent information about LangGraph for you. Let me summarize the key points: 1. LangGraph is part of the LangChain ecosystem, which is a leading platform in AI development. 2. Recent updates and features of LangGraph include: a. LangGraph Platform: This seems to be a cloud-based version of LangGraph, though specific details weren't provided in the search results. ... 3. Keep an eye on LangGraph Platform developments, as cloud-based solutions often provide an easier starting point for learners. 4. Consider how LangGraph fits into the broader LangChain ecosystem, especially its interaction with tools like LangSmith. Is there any specific aspect of LangGraph you'd like to know more about? I'd be happy to do a more focused search on particular features or use cases. Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings... ``` ```python events = graph.stream( { "messages": [ { "role": "user", "content": ( "Ya that's helpful. Maybe I'll " "build an autonomous agent with it!" ), }, ], }, config, stream_mode="values", ) for event in events: if "messages" in event: event["messages"][-1].pretty_print() ``` ``` ================================ Human Message ================================= Ya that's helpful. Maybe I'll build an autonomous agent with it! ================================== Ai Message ================================== [{'text': "That's an exciting idea! Building an autonomous agent with LangGraph is indeed a great application of this technology. LangGraph is particularly well-suited for creating complex, multi-step AI workflows, which is perfect for autonomous agents. Let me gather some more specific information about using LangGraph for building autonomous agents.", 'type': 'text'}, {'id': 'toolu_01QWNHhUaeeWcGXvA4eHT7Zo', 'input': {'query': 'Building autonomous agents with LangGraph examples and tutorials'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}] Tool Calls: tavily_search_results_json (toolu_01QWNHhUaeeWcGXvA4eHT7Zo) Call ID: toolu_01QWNHhUaeeWcGXvA4eHT7Zo Args: query: Building autonomous agents with LangGraph examples and tutorials ================================= Tool Message ================================= Name: tavily_search_results_json [{'url': 'https://towardsdatascience.com/building-autonomous-multi-tool-agents-with-gemini-2-0-and-langgraph-ad3d7bd5e79d', 'content': 'Building Autonomous Multi-Tool Agents with Gemini 2.0 and LangGraph | by Youness Mansar | Jan, 2025 | Towards Data Science Building Autonomous Multi-Tool Agents with Gemini 2.0 and LangGraph A practical tutorial with full code examples for building and running multi-tool agents Towards Data Science LLMs are remarkable — they can memorize vast amounts of information, answer general knowledge questions, write code, generate stories, and even fix your grammar. In this tutorial, we are going to build a simple LLM agent that is equipped with four tools that it can use to answer a user’s question. This Agent will have the following specifications: Follow Published in Towards Data Science --------------------------------- Your home for data science and AI. Follow Follow Follow'}, {'url': 'https://github.com/anmolaman20/Tools_and_Agents', 'content': 'GitHub - anmolaman20/Tools_and_Agents: This repository provides resources for building AI agents using Langchain and Langgraph. This repository provides resources for building AI agents using Langchain and Langgraph. This repository provides resources for building AI agents using Langchain and Langgraph. This repository serves as a comprehensive guide for building AI-powered agents using Langchain and Langgraph. It provides hands-on examples, practical tutorials, and resources for developers and AI enthusiasts to master building intelligent systems and workflows. AI Agent Development: Gain insights into creating intelligent systems that think, reason, and adapt in real time. This repository is ideal for AI practitioners, developers exploring language models, or anyone interested in building intelligent systems. This repository provides resources for building AI agents using Langchain and Langgraph.'}] ================================== Ai Message ================================== Great idea! Building an autonomous agent with LangGraph is definitely an exciting project. Based on the latest information I've found, here are some insights and tips for building autonomous agents with LangGraph: 1. Multi-Tool Agents: LangGraph is particularly well-suited for creating autonomous agents that can use multiple tools. This allows your agent to have a diverse set of capabilities and choose the right tool for each task. 2. Integration with Large Language Models (LLMs): You can combine LangGraph with powerful LLMs like Gemini 2.0 to create more intelligent and capable agents. The LLM can serve as the "brain" of your agent, making decisions and generating responses. 3. Workflow Management: LangGraph excels at managing complex, multi-step AI workflows. This is crucial for autonomous agents that need to break down tasks into smaller steps and execute them in the right order. ... 6. Pay attention to how you structure the agent's decision-making process and workflow. 7. Don't forget to implement proper error handling and safety measures, especially if your agent will be interacting with external systems or making important decisions. Building an autonomous agent is an iterative process, so be prepared to refine and improve your agent over time. Good luck with your project! If you need any more specific information as you progress, feel free to ask. Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings... ``` ## 3. Replay the full state history Now that you have added steps to the chatbot, you can `replay` the full state history to see everything that occurred. ```python to_replay = None for state in graph.get_state_history(config): print("Num Messages: ", len(state.values["messages"]), "Next: ", state.next) print("-" * 80) if len(state.values["messages"]) == 6: # We are somewhat arbitrarily selecting a specific state based on the number of chat messages in the state. to_replay = state ``` ``` Num Messages: 8 Next: () -------------------------------------------------------------------------------- Num Messages: 7 Next: ('chatbot',) -------------------------------------------------------------------------------- Num Messages: 6 Next: ('tools',) -------------------------------------------------------------------------------- Num Messages: 5 Next: ('chatbot',) -------------------------------------------------------------------------------- Num Messages: 4 Next: ('__start__',) -------------------------------------------------------------------------------- Num Messages: 4 Next: () -------------------------------------------------------------------------------- Num Messages: 3 Next: ('chatbot',) -------------------------------------------------------------------------------- Num Messages: 2 Next: ('tools',) -------------------------------------------------------------------------------- Num Messages: 1 Next: ('chatbot',) -------------------------------------------------------------------------------- Num Messages: 0 Next: ('__start__',) -------------------------------------------------------------------------------- ``` Checkpoints are saved for every step of the graph. This **spans invocations** so you can rewind across a full thread's history. ## Resume from a checkpoint Resume from the `to_replay` state, which is after the `chatbot` node in the second graph invocation. Resuming from this point will call the **action** node next. ```python print(to_replay.next) print(to_replay.config) ``` ``` ('tools',) {'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1efd43e3-0c1f-6c4e-8006-891877d65740'}} ``` ## 4. Load a state from a moment-in-time The checkpoint's `to_replay.config` contains a `checkpoint_id` timestamp. Providing this `checkpoint_id` value tells LangGraph's checkpointer to **load** the state from that moment in time. ```python # The `checkpoint_id` in the `to_replay.config` corresponds to a state we've persisted to our checkpointer. for event in graph.stream(None, to_replay.config, stream_mode="values"): if "messages" in event: event["messages"][-1].pretty_print() ``` ``` ================================== Ai Message ================================== [{'text': "That's an exciting idea! Building an autonomous agent with LangGraph is indeed a great application of this technology. LangGraph is particularly well-suited for creating complex, multi-step AI workflows, which is perfect for autonomous agents. Let me gather some more specific information about using LangGraph for building autonomous agents.", 'type': 'text'}, {'id': 'toolu_01QWNHhUaeeWcGXvA4eHT7Zo', 'input': {'query': 'Building autonomous agents with LangGraph examples and tutorials'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}] Tool Calls: tavily_search_results_json (toolu_01QWNHhUaeeWcGXvA4eHT7Zo) Call ID: toolu_01QWNHhUaeeWcGXvA4eHT7Zo Args: query: Building autonomous agents with LangGraph examples and tutorials ================================= Tool Message ================================= Name: tavily_search_results_json [{'url': 'https://towardsdatascience.com/building-autonomous-multi-tool-agents-with-gemini-2-0-and-langgraph-ad3d7bd5e79d', 'content': 'Building Autonomous Multi-Tool Agents with Gemini 2.0 and LangGraph | by Youness Mansar | Jan, 2025 | Towards Data Science Building Autonomous Multi-Tool Agents with Gemini 2.0 and LangGraph A practical tutorial with full code examples for building and running multi-tool agents Towards Data Science LLMs are remarkable — they can memorize vast amounts of information, answer general knowledge questions, write code, generate stories, and even fix your grammar. In this tutorial, we are going to build a simple LLM agent that is equipped with four tools that it can use to answer a user’s question. This Agent will have the following specifications: Follow Published in Towards Data Science --------------------------------- Your home for data science and AI. Follow Follow Follow'}, {'url': 'https://github.com/anmolaman20/Tools_and_Agents', 'content': 'GitHub - anmolaman20/Tools_and_Agents: This repository provides resources for building AI agents using Langchain and Langgraph. This repository provides resources for building AI agents using Langchain and Langgraph. This repository provides resources for building AI agents using Langchain and Langgraph. This repository serves as a comprehensive guide for building AI-powered agents using Langchain and Langgraph. It provides hands-on examples, practical tutorials, and resources for developers and AI enthusiasts to master building intelligent systems and workflows. AI Agent Development: Gain insights into creating intelligent systems that think, reason, and adapt in real time. This repository is ideal for AI practitioners, developers exploring language models, or anyone interested in building intelligent systems. This repository provides resources for building AI agents using Langchain and Langgraph.'}] ================================== Ai Message ================================== Great idea! Building an autonomous agent with LangGraph is indeed an excellent way to apply and deepen your understanding of the technology. Based on the search results, I can provide you with some insights and resources to help you get started: 1. Multi-Tool Agents: LangGraph is well-suited for building autonomous agents that can use multiple tools. This allows your agent to have a variety of capabilities and choose the appropriate tool based on the task at hand. 2. Integration with Large Language Models (LLMs): There's a tutorial that specifically mentions using Gemini 2.0 (Google's LLM) with LangGraph to build autonomous agents. This suggests that LangGraph can be integrated with various LLMs, giving you flexibility in choosing the language model that best fits your needs. 3. Practical Tutorials: There are tutorials available that provide full code examples for building and running multi-tool agents. These can be invaluable as you start your project, giving you a concrete starting point and demonstrating best practices. ... Remember, building an autonomous agent is an iterative process. Start simple and gradually increase complexity as you become more comfortable with LangGraph and its capabilities. Would you like more information on any specific aspect of building your autonomous agent with LangGraph? Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings... ``` The graph resumed execution from the `action` node. You can tell this is the case since the first value printed above is the response from our search engine tool. **Congratulations!** You've now used time-travel checkpoint traversal in LangGraph. Being able to rewind and explore alternative paths opens up a world of possibilities for debugging, experimentation, and interactive applications. ## Learn more Take your LangGraph journey further by exploring deployment and advanced features: * **[LangGraph Server quickstart](https://langchain-ai.github.io/langgraph/tutorials/langgraph-platform/local-server/)**: Launch a LangGraph server locally and interact with it using the REST API and LangGraph Studio Web UI. * **[LangGraph Platform quickstart](https://langchain-ai.github.io/langgraph/cloud/quick_start/)**: Deploy your LangGraph app using LangGraph Platform. * **[LangGraph Platform concepts](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/)**: Understand the foundational concepts of the LangGraph Platform.

---

# URL: https://langchain-ai.github.io/langgraph/tutorials/get-started/5-customize-state
**Depth**: 1

# Customize state In this tutorial, you will add additional fields to the state to define complex behavior without relying on the message list. The chatbot will use its search tool to find specific information and forward them to a human for review. Note: This tutorial builds on [Add human-in-the-loop controls](https://langchain-ai.github.io/langgraph/tutorials/get-started/4-human-in-the-loop/). ## 1. Add keys to the state Update the chatbot to research the birthday of an entity by adding `name` and `birthday` keys to the state: ```python from typing import Annotated from typing_extensions import TypedDict from langgraph.graph.message import add_messages class State(TypedDict): messages: Annotated[list, add_messages] name: str birthday: str ``` Adding this information to the state makes it easily accessible by other graph nodes (like a downstream node that stores or processes the information), as well as the graph's persistence layer. ## 2. Update the state inside the tool Now, populate the state keys inside of the `human_assistance` tool. This allows a human to review the information before it is stored in the state. Use `Command` to issue a state update from inside the tool. ```python from langchain_core.messages import ToolMessage from langchain_core.tools import InjectedToolCallId, tool from langgraph.types import Command, interrupt @tool # Note that because we are generating a ToolMessage for a state update, we # generally require the ID of the corresponding tool call. We can use # LangChain's InjectedToolCallId to signal that this argument should not # be revealed to the model in the tool's schema. def human_assistance( name: str, birthday: str, tool_call_id: Annotated[str, InjectedToolCallId] ) -> str: """Request assistance from a human.""" human_response = interrupt( { "question": "Is this correct?", "name": name, "birthday": birthday, }, ) # If the information is correct, update the state as-is. if human_response.get("correct", "").lower().startswith("y"): verified_name = name verified_birthday = birthday response = "Correct" # Otherwise, receive information from the human reviewer. else: verified_name = human_response.get("name", name) verified_birthday = human_response.get("birthday", birthday) response = f"Made a correction: {human_response}" # This time we explicitly update the state with a ToolMessage inside # the tool. state_update = { "name": verified_name, "birthday": verified_birthday, "messages": [ToolMessage(response, tool_call_id=tool_call_id)], } # We return a Command object in the tool to update our state. return Command(update=state_update) ``` The rest of the graph stays the same. ## 3. Prompt the chatbot Prompt the chatbot to look up the "birthday" of the LangGraph library and direct the chatbot to reach out to the `human_assistance` tool once it has the required information. By setting `name` and `birthday` in the arguments for the tool, you force the chatbot to generate proposals for these fields. ```python user_input = ( "Can you look up when LangGraph was released? " "When you have the answer, use the human_assistance tool for review." ) config = {"configurable": {"thread_id": "1"}} events = graph.stream( {"messages": [{"role": "user", "content": user_input}]}, config, stream_mode="values", ) for event in events: if "messages" in event: event["messages"][-1].pretty_print() ``` ```text ================================ Human Message ================================= Can you look up when LangGraph was released? When you have the answer, use the human_assistance tool for review. ================================== Ai Message ================================== [{'text': "Certainly! I'll start by searching for information about LangGraph's release date using the Tavily search function. Then, I'll use the human_assistance tool for review.", 'type': 'text'}, {'id': 'toolu_01JoXQPgTVJXiuma8xMVwqAi', 'input': {'query': 'LangGraph release date'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}] Tool Calls: tavily_search_results_json (toolu_01JoXQPgTVJXiuma8xMVwqAi) Call ID: toolu_01JoXQPgTVJXiuma8xMVwqAi Args: query: LangGraph release date ================================= Tool Message ================================= Name: tavily_search_results_json [{'url': 'https://blog.langchain.dev/langgraph-cloud/', 'content': "We also have a new stable release of LangGraph. By LangChain 6 min read Jun 27, 2024 (Oct '24) Edit: Since the launch of LangGraph Platform, we now have multiple deployment options alongside LangGraph Studio - which now fall under LangGraph Platform. LangGraph Platform is synonymous with our Cloud SaaS deployment option."}, {'url': 'https://changelog.langchain.com/announcements/langgraph-cloud-deploy-at-scale-monitor-carefully-iterate-boldly', 'content': 'LangChain - Changelog | ☁ 🚀 LangGraph Platform: Deploy at scale, monitor LangChain LangSmith LangGraph LangChain LangSmith LangGraph LangChain Changelog Sign up for our newsletter to stay up to date DATE: The LangChain Team LangGraph LangGraph Platform ☁ 🚀 LangGraph Platform: Deploy at scale, monitor carefully, iterate boldly DATE: June 27, 2024 AUTHOR: The LangChain Team LangGraph Platform is now in closed beta, offering scalable, fault-tolerant deployment for LangGraph agents. LangGraph Platform also includes a new playground-like studio for debugging agent failure modes and quick iteration: Join the waitlist today for LangGraph Platform. And to learn more, read our blog post announcement or check out our docs. Subscribe By clicking subscribe, you accept our privacy policy and terms and conditions.'}] ================================== Ai Message ================================== [{'text': "Based on the search results, it appears that LangGraph was already in existence before June 27, 2024, when LangGraph Platform was announced. However, the search results don't provide a specific release date for the original LangGraph. \n\nGiven this information, I'll use the human_assistance tool to review and potentially provide more accurate information about LangGraph's initial release date.", 'type': 'text'}, {'id': 'toolu_01JDQAV7nPqMkHHhNs3j3XoN', 'input': {'name': 'Assistant', 'birthday': '2023-01-01'}, 'name': 'human_assistance', 'type': 'tool_use'}] Tool Calls: human_assistance (toolu_01JDQAV7nPqMkHHhNs3j3XoN) Call ID: toolu_01JDQAV7nPqMkHHhNs3j3XoN Args: name: Assistant birthday: 2023-01-01 ``` We've hit the `interrupt` in the `human_assistance` tool again. ## 4. Add human assistance The chatbot failed to identify the correct date, so supply it with information: ```python human_command = Command( resume={ "name": "LangGraph", "birthday": "Jan 17, 2024", }, ) events = graph.stream(human_command, config, stream_mode="values") for event in events: if "messages" in event: event["messages"][-1].pretty_print() ``` ```text ================================== Ai Message ================================== [{'text': "Based on the search results, it appears that LangGraph was already in existence before June 27, 2024, when LangGraph Platform was announced. However, the search results don't provide a specific release date for the original LangGraph. \n\nGiven this information, I'll use the human_assistance tool to review and potentially provide more accurate information about LangGraph's initial release date.", 'type': 'text'}, {'id': 'toolu_01JDQAV7nPqMkHHhNs3j3XoN', 'input': {'name': 'Assistant', 'birthday': '2023-01-01'}, 'name': 'human_assistance', 'type': 'tool_use'}] Tool Calls: human_assistance (toolu_01JDQAV7nPqMkHHhNs3j3XoN) Call ID: toolu_01JDQAV7nPqMkHHhNs3j3XoN Args: name: Assistant birthday: 2023-01-01 ================================= Tool Message ================================= Name: human_assistance Made a correction: {'name': 'LangGraph', 'birthday': 'Jan 17, 2024'} ================================== Ai Message ================================== Thank you for the human assistance. I can now provide you with the correct information about LangGraph's release date. LangGraph was initially released on January 17, 2024. This information comes from the human assistance correction, which is more accurate than the search results I initially found. To summarize: 1. LangGraph's original release date: January 17, 2024 2. LangGraph Platform announcement: June 27, 2024 It's worth noting that LangGraph had been in development and use for some time before the LangGraph Platform announcement, but the official initial release of LangGraph itself was on January 17, 2024. ``` Note that these fields are now reflected in the state: ```python snapshot = graph.get_state(config) {k: v for k, v in snapshot.values.items() if k in ("name", "birthday")} ``` ```text {'name': 'LangGraph', 'birthday': 'Jan 17, 2024'} ``` This makes them easily accessible to downstream nodes (e.g., a node that further processes or stores the information). ## 5. Manually update the state LangGraph gives a high degree of control over the application state. For instance, at any point (including when interrupted), you can manually override a key using `graph.update_state`: ```python graph.update_state(config, {"name": "LangGraph (library)"}) ``` ```text {'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1efd4ec5-cf69-6352-8006-9278f1730162'}} ``` ## 6. View the new value If you call `graph.get_state`, you can see the new value is reflected: ```python snapshot = graph.get_state(config) {k: v for k, v in snapshot.values.items() if k in ("name", "birthday")} ``` ```text {'name': 'LangGraph (library)', 'birthday': 'Jan 17, 2024'} ``` Manual state updates will generate a trace in LangSmith. If desired, they can also be used to control human-in-the-loop workflows. Use of the `interrupt` function is generally recommended instead, as it allows data to be transmitted in a human-in-the-loop interaction independently of state updates. **Congratulations!** You've added custom keys to the state to facilitate a more complex workflow, and learned how to generate state updates from inside tools. Check out the code snippet below to review the graph from this tutorial: ```python pip install -U "langchain[openai]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["OPENAI_API_KEY"] = "sk-..." llm = init_chat_model("openai:gpt-4.1") ``` ```python pip install -U "langchain[anthropic]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["ANTHROPIC_API_KEY"] = "sk-..." llm = init_chat_model("anthropic:claude-3-5-sonnet-latest") ``` ```python pip install -U "langchain[openai]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["AZURE_OPENAI_API_KEY"] = "..." os.environ["AZURE_OPENAI_ENDPOINT"] = "..." os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview" llm = init_chat_model( "azure_openai:gpt-4.1", azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"], ) ``` ```python pip install -U "langchain[google-genai]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["GOOGLE_API_KEY"] = "..." llm = init_chat_model("google_genai:gemini-2.0-flash") ``` ```python pip install -U "langchain[aws]" ``` ```python from langchain.chat_models import init_chat_model # Follow the steps here to configure your credentials: # https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html llm = init_chat_model( "anthropic.claude-3-5-sonnet-20240620-v1:0", model_provider="bedrock_converse", ) ``` ```python from typing import Annotated from langchain_tavily import TavilySearch from langchain_core.messages import ToolMessage from langchain_core.tools import InjectedToolCallId, tool from typing_extensions import TypedDict from langgraph.checkpoint.memory import MemorySaver from langgraph.graph import StateGraph, START, END from langgraph.graph.message import add_messages from langgraph.prebuilt import ToolNode, tools_condition from langgraph.types import Command, interrupt class State(TypedDict): messages: Annotated[list, add_messages] name: str birthday: str @tool def human_assistance( name: str, birthday: str, tool_call_id: Annotated[str, InjectedToolCallId] ) -> str: """Request assistance from a human.""" human_response = interrupt( { "question": "Is this correct?", "name": name, "birthday": birthday, }, ) if human_response.get("correct", "").lower().startswith("y"): verified_name = name verified_birthday = birthday response = "Correct" else: verified_name = human_response.get("name", name) verified_birthday = human_response.get("birthday", birthday) response = f"Made a correction: {human_response}" state_update = { "name": verified_name, "birthday": verified_birthday, "messages": [ToolMessage(response, tool_call_id=tool_call_id)], } return Command(update=state_update) tool = TavilySearch(max_results=2) tools = [tool, human_assistance] llm_with_tools = llm.bind_tools(tools) def chatbot(state: State): message = llm_with_tools.invoke(state["messages"]) assert(len(message.tool_calls) <= 1) return {"messages": [message]} graph_builder = StateGraph(State) graph_builder.add_node("chatbot", chatbot) tool_node = ToolNode(tools=tools) graph_builder.add_node("tools", tool_node) graph_builder.add_conditional_edges( "chatbot", tools_condition, ) graph_builder.add_edge("tools", "chatbot") graph_builder.add_edge(START, "chatbot") memory = MemorySaver() graph = graph_builder.compile(checkpointer=memory) ``` ## Next steps There's one more concept to review before finishing the LangGraph basics tutorials: connecting `checkpointing` and `state updates` to [time travel](https://langchain-ai.github.io/langgraph/tutorials/get-started/6-time-travel/).

---

# URL: https://langchain-ai.github.io/langgraph/tutorials/get-started/4-human-in-the-loop
**Depth**: 1

# Add human-in-the-loop controls Agents can be unreliable and may need human input to successfully accomplish tasks. Similarly, for some actions, you may want to require human approval before running to ensure that everything is running as intended. LangGraph's [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) layer supports **human-in-the-loop** workflows, allowing execution to pause and resume based on user feedback. The primary interface to this functionality is the [`interrupt`](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/add-human-in-the-loop/) function. Calling `interrupt` inside a node will pause execution. Execution can be resumed, together with new input from a human, by passing in a [Command](https://langchain-ai.github.io/langgraph/concepts/low_level/#command). `interrupt` is ergonomically similar to Python's built-in `input()`, [with some caveats](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/add-human-in-the-loop/). Note: This tutorial builds on [Add memory](https://langchain-ai.github.io/langgraph/tutorials/get-started/3-add-memory/). ## 1. Add the `human_assistance` tool Starting with the existing code from the [Add memory to the chatbot](https://langchain-ai.github.io/langgraph/tutorials/get-started/3-add-memory/) tutorial, add the `human_assistance` tool to the chatbot. This tool uses `interrupt` to receive information from a human. Let's first select a chat model: ```bash pip install -U "langchain[openai]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["OPENAI_API_KEY"] = "sk-..." llm = init_chat_model("openai:gpt-4.1") ``` 👉 Read the [OpenAI integration docs](https://python.langchain.com/docs/integrations/chat/openai/) ```bash pip install -U "langchain[anthropic]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["ANTHROPIC_API_KEY"] = "sk-..." llm = init_chat_model("anthropic:claude-3-5-sonnet-latest") ``` 👉 Read the [Anthropic integration docs](https://python.langchain.com/docs/integrations/chat/anthropic/) ```bash pip install -U "langchain[openai]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["AZURE_OPENAI_API_KEY"] = "..." os.environ["AZURE_OPENAI_ENDPOINT"] = "..." os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview" llm = init_chat_model( "azure_openai:gpt-4.1", azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"], ) ``` 👉 Read the [Azure integration docs](https://python.langchain.com/docs/integrations/chat/azure_chat_openai/) ```bash pip install -U "langchain[google-genai]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["GOOGLE_API_KEY"] = "..." llm = init_chat_model("google_genai:gemini-2.0-flash") ``` 👉 Read the [Google GenAI integration docs](https://python.langchain.com/docs/integrations/chat/google_generative_ai/) ```bash pip install -U "langchain[aws]" ``` ```python from langchain.chat_models import init_chat_model # Follow the steps here to configure your credentials: # https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html llm = init_chat_model( "anthropic.claude-3-5-sonnet-20240620-v1:0", model_provider="bedrock_converse", ) ``` 👉 Read the [AWS Bedrock integration docs](https://python.langchain.com/docs/integrations/chat/bedrock/) We can now incorporate it into our `StateGraph` with an additional tool: ```python from typing import Annotated from langchain_tavily import TavilySearch from langchain_core.tools import tool from typing_extensions import TypedDict from langgraph.checkpoint.memory import MemorySaver from langgraph.graph import StateGraph, START, END from langgraph.graph.message import add_messages from langgraph.prebuilt import ToolNode, tools_condition from langgraph.types import Command, interrupt class State(TypedDict): messages: Annotated[list, add_messages] graph_builder = StateGraph(State) @tool def human_assistance(query: str) -> str: """Request assistance from a human.""" human_response = interrupt({"query": query}) return human_response["data"] tool = TavilySearch(max_results=2) tools = [tool, human_assistance] llm_with_tools = llm.bind_tools(tools) def chatbot(state: State): message = llm_with_tools.invoke(state["messages"]) # Because we will be interrupting during tool execution, # we disable parallel tool calling to avoid repeating any # tool invocations when we resume. assert len(message.tool_calls) <= 1 return {"messages": [message]} graph_builder.add_node("chatbot", chatbot) tool_node = ToolNode(tools=tools) graph_builder.add_node("tools", tool_node) graph_builder.add_conditional_edges( "chatbot", tools_condition, ) graph_builder.add_edge("tools", "chatbot") graph_builder.add_edge(START, "chatbot") ``` Tip: For more information and examples of human-in-the-loop workflows, see [Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/). ## 2. Compile the graph We compile the graph with a checkpointer, as before: ```python memory = MemorySaver() graph = graph_builder.compile(checkpointer=memory) ``` ## 3. Visualize the graph (optional) Visualizing the graph, you get the same layout as before – just with the added tool! ```python from IPython.display import Image, display try: display(Image(graph.get_graph().draw_mermaid_png())) except Exception: # This requires some extra dependencies and is optional pass ``` ## 4. Prompt the chatbot Now, prompt the chatbot with a question that will engage the new `human_assistance` tool: ```python user_input = "I need some expert guidance for building an AI agent. Could you request assistance for me?" config = {"configurable": {"thread_id": "1"}} events = graph.stream( {"messages": [{"role": "user", "content": user_input}]}, config, stream_mode="values", ) for event in events: if "messages" in event: event["messages"][-1].pretty_print() ``` ``` ================================ Human Message ================================= I need some expert guidance for building an AI agent. Could you request assistance for me? ================================== Ai Message ================================== [{'text': "Certainly! I'd be happy to request expert assistance for you regarding building an AI agent. To do this, I'll use the human_assistance function to relay your request. Let me do that for you now.", 'type': 'text'}, {'id': 'toolu_01ABUqneqnuHNuo1vhfDFQCW', 'input': {'query': 'A user is requesting expert guidance for building an AI agent. Could you please provide some expert advice or resources on this topic?'}, 'name': 'human_assistance', 'type': 'tool_use'}] Tool Calls: human_assistance (toolu_01ABUqneqnuHNuo1vhfDFQCW) Call ID: toolu_01ABUqneqnuHNuo1vhfDFQCW Args: query: A user is requesting expert guidance for building an AI agent. Could you please provide some expert advice or resources on this topic? ``` The chatbot generated a tool call, but then execution has been interrupted. If you inspect the graph state, you see that it stopped at the tools node: ```python snapshot = graph.get_state(config) snapshot.next ``` ``` ('tools',) ``` Info: Take a closer look at the `human_assistance` tool: ```python @tool def human_assistance(query: str) -> str: """Request assistance from a human.""" human_response = interrupt({"query": query}) return human_response["data"] ``` Similar to Python's built-in `input()` function, calling `interrupt` inside the tool will pause execution. Progress is persisted based on the [checkpointer](https://langchain-ai.github.io/langgraph/concepts/persistence/#checkpointer-libraries); so if it is persisting with Postgres, it can resume at any time as long as the database is alive. In this example, it is persisting with the in-memory checkpointer and can resume any time if the Python kernel is running. ## 5. Resume execution To resume execution, pass a [Command](https://langchain-ai.github.io/langgraph/concepts/low_level/#command) object containing data expected by the tool. The format of this data can be customized based on needs. For this example, use a dict with a key `"data"`: ```python human_response = ( "We, the experts are here to help! We'd recommend you check out LangGraph to build your agent." " It's much more reliable and extensible than simple autonomous agents." ) human_command = Command(resume={"data": human_response}) events = graph.stream(human_command, config, stream_mode="values") for event in events: if "messages" in event: event["messages"][-1].pretty_print() ``` ``` ================================== Ai Message ================================== [{'text': "Certainly! I'd be happy to request expert assistance for you regarding building an AI agent. To do this, I'll use the human_assistance function to relay your request. Let me do that for you now.", 'type': 'text'}, {'id': 'toolu_01ABUqneqnuHNuo1vhfDFQCW', 'input': {'query': 'A user is requesting expert guidance for building an AI agent. Could you please provide some expert advice or resources on this topic?'}, 'name': 'human_assistance', 'type': 'tool_use'}] Tool Calls: human_assistance (toolu_01ABUqneqnuHNuo1vhfDFQCW) Call ID: toolu_01ABUqneqnuHNuo1vhfDFQCW Args: query: A user is requesting expert guidance for building an AI agent. Could you please provide some expert advice or resources on this topic? ================================= Tool Message ================================= Name: human_assistance We, the experts are here to help! We'd recommend you check out LangGraph to build your agent. It's much more reliable and extensible than simple autonomous agents. ================================== Ai Message ================================== Thank you for your patience. I've received some expert advice regarding your request for guidance on building an AI agent. Here's what the experts have suggested: The experts recommend that you look into LangGraph for building your AI agent. They mention that LangGraph is a more reliable and extensible option compared to simple autonomous agents. LangGraph is likely a framework or library designed specifically for creating AI agents with advanced capabilities. Here are a few points to consider based on this recommendation: 1. Reliability: The experts emphasize that LangGraph is more reliable than simpler autonomous agent approaches. This could mean it has better stability, error handling, or consistent performance. 2. Extensibility: LangGraph is described as more extensible, which suggests that it probably offers a flexible architecture that allows you to easily add new features or modify existing ones as your agent's requirements evolve. 3. Advanced capabilities: Given that it's recommended over "simple autonomous agents," LangGraph likely provides more sophisticated tools and techniques for building complex AI agents. ... 2. Look for tutorials or guides specifically focused on building AI agents with LangGraph. 3. Check if there are any community forums or discussion groups where you can ask questions and get support from other developers using LangGraph. If you'd like more specific information about LangGraph or have any questions about this recommendation, please feel free to ask, and I can request further assistance from the experts. Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings... ``` The input has been received and processed as a tool message. Review this call's [LangSmith trace](https://smith.langchain.com/public/9f0f87e3-56a7-4dde-9c76-b71675624e91/r) to see the exact work that was done in the above call. Notice that the state is loaded in the first step so that our chatbot can continue where it left off. **Congratulations!** You've used an `interrupt` to add human-in-the-loop execution to your chatbot, allowing for human oversight and intervention when needed. This opens up the potential UIs you can create with your AI systems. Since you have already added a **checkpointer**, as long as the underlying persistence layer is running, the graph can be paused **indefinitely** and resumed at any time as if nothing had happened. Check out the code snippet below to review the graph from this tutorial: ```bash pip install -U "langchain[openai]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["OPENAI_API_KEY"] = "sk-..." llm = init_chat_model("openai:gpt-4.1") ``` 👉 Read the [OpenAI integration docs](https://python.langchain.com/docs/integrations/chat/openai/) ```bash pip install -U "langchain[anthropic]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["ANTHROPIC_API_KEY"] = "sk-..." llm = init_chat_model("anthropic:claude-3-5-sonnet-latest") ``` 👉 Read the [Anthropic integration docs](https://python.langchain.com/docs/integrations/chat/anthropic/) ```bash pip install -U "langchain[openai]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["AZURE_OPENAI_API_KEY"] = "..." os.environ["AZURE_OPENAI_ENDPOINT"] = "..." os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview" llm = init_chat_model( "azure_openai:gpt-4.1", azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"], ) ``` 👉 Read the [Azure integration docs](https://python.langchain.com/docs/integrations/chat/azure_chat_openai/) ```bash pip install -U "langchain[google-genai]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["GOOGLE_API_KEY"] = "..." llm = init_chat_model("google_genai:gemini-2.0-flash") ``` 👉 Read the [Google GenAI integration docs](https://python.langchain.com/docs/integrations/chat/google_generative_ai/) ```bash pip install -U "langchain[aws]" ``` ```python from langchain.chat_models import init_chat_model # Follow the steps here to configure your credentials: # https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html llm = init_chat_model( "anthropic.claude-3-5-sonnet-20240620-v1:0", model_provider="bedrock_converse", ) ``` 👉 Read the [AWS Bedrock integration docs](https://python.langchain.com/docs/integrations/chat/bedrock/) ```python from typing import Annotated from langchain_tavily import TavilySearch from langchain_core.tools import tool from typing_extensions import TypedDict from langgraph.checkpoint.memory import MemorySaver from langgraph.graph import StateGraph, START, END from langgraph.graph.message import add_messages from langgraph.prebuilt import ToolNode, tools_condition from langgraph.types import Command, interrupt class State(TypedDict): messages: Annotated[list, add_messages] graph_builder = StateGraph(State) @tool def human_assistance(query: str) -> str: """Request assistance from a human.""" human_response = interrupt({"query": query}) return human_response["data"] tool = TavilySearch(max_results=2) tools = [tool, human_assistance] llm_with_tools = llm.bind_tools(tools) def chatbot(state: State): message = llm_with_tools.invoke(state["messages"]) assert(len(message.tool_calls) <= 1) return {"messages": [message]} graph_builder.add_node("chatbot", chatbot) tool_node = ToolNode(tools=tools) graph_builder.add_node("tools", tool_node) graph_builder.add_conditional_edges( "chatbot", tools_condition, ) graph_builder.add_edge("tools", "chatbot") graph_builder.add_edge(START, "chatbot") memory = MemorySaver() graph = graph_builder.compile(checkpointer=memory) ``` ## Next steps So far, the tutorial examples have relied on a simple state with one entry: a list of messages. You can go far with this simple state, but if you want to define complex behavior without relying on the message list, you can [add additional fields to the state](https://langchain-ai.github.io/langgraph/tutorials/get-started/5-customize-state/).

---

