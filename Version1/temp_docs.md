# URL: https://langchain-ai.github.io/langgraph/tutorials/get-started/1-build-basic-chatbot/
**Depth**: 0

# Build a basic chatbot In this tutorial, you will build a basic chatbot. This chatbot is the basis for the following series of tutorials where you will progressively add more sophisticated capabilities, and be introduced to key LangGraph concepts along the way. Letâ€™s dive in! ðŸŒŸ ## Prerequisites Before you start this tutorial, ensure you have access to a LLM that supports tool-calling features, such as [OpenAI](https://platform.openai.com/api-keys), [Anthropic](https://console.anthropic.com/settings/keys), or [Google Gemini](https://ai.google.dev/gemini-api/docs/api-key). ## 1. Install packages Install the required packages: ```bash pip install -U langgraph langsmith ``` Tip: Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph. For more information on how to get started, see [LangSmith docs](https://docs.smith.langchain.com). ## 2. Create a `StateGraph` Now you can create a basic chatbot using LangGraph. This chatbot will respond directly to user messages. Start by creating a `StateGraph`. A `StateGraph` object defines the structure of our chatbot as a "state machine". We'll add `nodes` to represent the llm and functions our chatbot can call and `edges` to specify how the bot should transition between these functions. API Reference: [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) | [START](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START) | [END](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.END) | [add_messages](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages) ```python from typing import Annotated from typing_extensions import TypedDict from langgraph.graph import StateGraph, START, END from langgraph.graph.message import add_messages class State(TypedDict): # Messages have the type "list". The `add_messages` function # in the annotation defines how this state key should be updated # (in this case, it appends messages to the list, rather than overwriting them) messages: Annotated[list, add_messages] graph_builder = StateGraph(State) ``` Our graph can now handle two key tasks: 1. Each `node` can receive the current `State` as input and output an update to the state. 2. Updates to `messages` will be appended to the existing list rather than overwriting it, thanks to the prebuilt [`add_messages`](https://langchain-ai.github.io/langgraph/reference/graphs/?h=add+messages#add_messages) function used with the `Annotated` syntax. Concept: When defining a graph, the first step is to define its `State`. The `State` includes the graph's schema and [reducer functions](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers) that handle state updates. In our example, `State` is a `TypedDict` with one key: `messages`. The [`add_messages`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages) reducer function is used to append new messages to the list instead of overwriting it. Keys without a reducer annotation will overwrite previous values. To learn more about state, reducers, and related concepts, see [LangGraph reference docs](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages). ## 3. Add a node Next, add a "`chatbot`" node. **Nodes** represent units of work and are typically regular Python functions. Let's first select a chat model: **OpenAI** ```bash pip install -U "langchain[openai]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["OPENAI_API_KEY"] = "sk-..." llm = init_chat_model("openai:gpt-4.1") ``` ðŸ‘‰ Read the [OpenAI integration docs](https://python.langchain.com/docs/integrations/chat/openai/) **Anthropic** ```bash pip install -U "langchain[anthropic]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["ANTHROPIC_API_KEY"] = "sk-..." llm = init_chat_model("anthropic:claude-3-5-sonnet-latest") ``` ðŸ‘‰ Read the [Anthropic integration docs](https://python.langchain.com/docs/integrations/chat/anthropic/) **Azure** ```bash pip install -U "langchain[openai]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["AZURE_OPENAI_API_KEY"] = "..." os.environ["AZURE_OPENAI_ENDPOINT"] = "..." os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview" llm = init_chat_model( "azure_openai:gpt-4.1", azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"], ) ``` ðŸ‘‰ Read the [Azure integration docs](https://python.langchain.com/docs/integrations/chat/azure_chat_openai/) **Google Gemini** ```bash pip install -U "langchain[google-genai]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["GOOGLE_API_KEY"] = "..." llm = init_chat_model("google_genai:gemini-2.0-flash") ``` ðŸ‘‰ Read the [Google GenAI integration docs](https://python.langchain.com/docs/integrations/chat/google_generative_ai/) **AWS Bedrock** ```bash pip install -U "langchain[aws]" ``` ```python from langchain.chat_models import init_chat_model # Follow the steps here to configure your credentials: # https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html llm = init_chat_model( "anthropic.claude-3-5-sonnet-20240620-v1:0", model_provider="bedrock_converse", ) ``` ðŸ‘‰ Read the [AWS Bedrock integration docs](https://python.langchain.com/docs/integrations/chat/bedrock/) We can now incorporate the chat model into a simple node: ```python def chatbot(state: State): return {"messages": [llm.invoke(state["messages"])]} # The first argument is the unique node name # The second argument is the function or object that will be called whenever # the node is used. graph_builder.add_node("chatbot", chatbot) ``` Notice how the `chatbot` node function takes the current `State` as input and returns a dictionary containing an updated `messages` list under the key "messages". This is the basic pattern for all LangGraph node functions. The `add_messages` function in our `State` will append the LLM's response messages to whatever messages are already in the state. ## 4. Add an `entry` point Add an `entry` point to tell the graph **where to start its work** each time it is run: ```python graph_builder.add_edge(START, "chatbot") ``` ## 5. Add an `exit` point Add an `exit` point to indicate **where the graph should finish execution**. This is helpful for more complex flows, but even in a simple graph like this, adding an end node improves clarity. ```python graph_builder.add_edge("chatbot", END) ``` This tells the graph to terminate after running the chatbot node. ## 6. Compile the graph Before running the graph, we'll need to compile it. We can do so by calling `compile()` on the graph builder. This creates a `CompiledStateGraph` we can invoke on our state. ```python graph = graph_builder.compile() ``` ## 7. Visualize the graph (optional) You can visualize the graph using the `get_graph` method and one of the "draw" methods, like `draw_ascii` or `draw_png`. The `draw` methods each require additional dependencies. ```python from IPython.display import Image, display try: display(Image(graph.get_graph().draw_mermaid_png())) except Exception: # This requires some extra dependencies and is optional pass ``` ## 8. Run the chatbot Now run the chatbot! Tip: You can exit the chat loop at any time by typing `quit`, `exit`, or `q`. ```python def stream_graph_updates(user_input: str): for event in graph.stream({"messages": [{"role": "user", "content": user_input}]}): for value in event.values(): print("Assistant:", value["messages"][-1].content) while True: try: user_input = input("User: ") if user_input.lower() in ["quit", "exit", "q"]: print("Goodbye!") break stream_graph_updates(user_input) except: # fallback if input() is not available user_input = "What do you know about LangGraph?" print("User: " + user_input) stream_graph_updates(user_input) break ``` ```text Assistant: LangGraph is a library designed to help build stateful multi-agent applications using language models. It provides tools for creating workflows and state machines to coordinate multiple AI agents or language model interactions. LangGraph is built on top of LangChain, leveraging its components while adding graph-based coordination capabilities. It's particularly useful for developing more complex, stateful AI applications that go beyond simple query-response interactions. Goodbye! ``` **Congratulations!** You've built your first chatbot using LangGraph. This bot can engage in basic conversation by taking user input and generating responses using an LLM. You can inspect a [LangSmith Trace](https://smith.langchain.com/public/7527e308-9502-4894-b347-f34385740d5a/r) for the call above. Below is the full code for this tutorial: API Reference: [init_chat_model](https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) | [START](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START) | [END](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.END) | [add_messages](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages) ```python from typing import Annotated from langchain.chat_models import init_chat_model from typing_extensions import TypedDict from langgraph.graph import StateGraph, START, END from langgraph.graph.message import add_messages class State(TypedDict): messages: Annotated[list, add_messages] graph_builder = StateGraph(State) llm = init_chat_model("anthropic:claude-3-5-sonnet-latest") def chatbot(state: State): return {"messages": [llm.invoke(state["messages"])]} # The first argument is the unique node name # The second argument is the function or object that will be called whenever # the node is used. graph_builder.add_node("chatbot", chatbot) graph_builder.add_edge(START, "chatbot") graph_builder.add_edge("chatbot", END) graph = graph_builder.compile() ``` ## Next steps You may have noticed that the bot's knowledge is limited to what's in its training data. In the next part, we'll [add a web search tool](https://langchain-ai.github.io/langgraph/tutorials/get-started/2-add-tools/) to expand the bot's knowledge and make it more capable.

---

# URL: https://langchain-ai.github.io/langgraph
**Depth**: 1

# LangGraph LangGraph is a low-level orchestration framework for building, managing, and deploying long-running, stateful agents. ## Get started Install LangGraph: ```bash pip install -U langgraph ``` Then, create an agent using prebuilt components: ```python # pip install -qU "langchain[anthropic]" to call the model from langgraph.prebuilt import create_react_agent def get_weather(city: str) -> str: """Get weather for a given city.""" return f"It's always sunny in {city}!" agent = create_react_agent( model="anthropic:claude-3-7-sonnet-latest", tools=[get_weather], prompt="You are a helpful assistant" ) # Run the agent agent.invoke( {"messages": [{"role": "user", "content": "what is the weather in sf"}]} ) ``` For more information, see the [Quickstart](https://langchain-ai.github.io/langgraph/agents/agents/). Or, to learn how to build an [agent workflow](https://langchain-ai.github.io/langgraph/concepts/low_level/) with a customizable architecture, long-term memory, and other complex task handling, see the [LangGraph basics tutorials](https://langchain-ai.github.io/langgraph/tutorials/get-started/1-build-basic-chatbot/). ## Core benefits LangGraph provides low-level supporting infrastructure for *any* long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits: * [Durable execution](https://langchain-ai.github.io/langgraph/concepts/durable_execution/): Build agents that persist through failures and can run for extended periods, automatically resuming from exactly where they left off. * [Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/): Seamlessly incorporate human oversight by inspecting and modifying agent state at any point during execution. * [Comprehensive memory](https://langchain-ai.github.io/langgraph/concepts/memory/): Create truly stateful agents with both short-term working memory for ongoing reasoning and long-term persistent memory across sessions. * [Debugging with LangSmith](http://www.langchain.com/langsmith): Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics. * [Production-ready deployment](https://langchain-ai.github.io/langgraph/concepts/deployment_options/): Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows. ## LangGraphâ€™s ecosystem While LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with: * [LangSmith](http://www.langchain.com/langsmith) â€” Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time. * [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/) â€” Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams â€” and iterate quickly with visual prototyping in [LangGraph Studio](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/). * [LangChain](https://python.langchain.com/docs/introduction/) â€“ Provides integrations and composable components to streamline LLM application development. > Note > > Looking for the JS version of LangGraph? See the [JS repo](https://github.com/langchain-ai/langgraphjs) and the [JS docs](https://langchain-ai.github.io/langgraphjs/). ## Additional resources * [Guides](https://langchain-ai.github.io/langgraph/how-tos/): Quick, actionable code snippets for topics such as streaming, adding memory & persistence, and design patterns (e.g. branching, subgraphs, etc.). * [Reference](https://langchain-ai.github.io/langgraph/reference/graphs/): Detailed reference on core classes, methods, how to use the graph and checkpointing APIs, and higher-level prebuilt components. * [Examples](https://langchain-ai.github.io/langgraph/tutorials/overview/): Guided examples on getting started with LangGraph. * [LangChain Academy](https://academy.langchain.com/courses/intro-to-langgraph): Learn the basics of LangGraph in our free, structured course. * [Templates](https://langchain-ai.github.io/langgraph/concepts/template_applications/): Pre-built reference apps for common agentic workflows (e.g. ReAct agent, memory, retrieval etc.) that can be cloned and adapted. * [Case studies](https://www.langchain.com/built-with-langgraph): Hear how industry leaders use LangGraph to ship AI applications at scale. ## Acknowledgements LangGraph is inspired by [Pregel](https://research.google/pubs/pub37252/) and [Apache Beam](https://beam.apache.org/). The public interface draws inspiration from [NetworkX](https://networkx.org/documentation/latest/). LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.

---

# URL: https://langchain-ai.github.io/langgraph/tutorials/get-started/1-build-basic-chatbot
**Depth**: 1

# Build a basic chatbot In this tutorial, you will build a basic chatbot. This chatbot is the basis for the following series of tutorials where you will progressively add more sophisticated capabilities, and be introduced to key LangGraph concepts along the way. Letâ€™s dive in! ðŸŒŸ ## Prerequisites Before you start this tutorial, ensure you have access to a LLM that supports tool-calling features, such as [OpenAI](https://platform.openai.com/api-keys), [Anthropic](https://console.anthropic.com/settings/keys), or [Google Gemini](https://ai.google.dev/gemini-api/docs/api-key). ## 1. Install packages Install the required packages: ```bash pip install -U langgraph langsmith ``` Tip: Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph. For more information on how to get started, see [LangSmith docs](https://docs.smith.langchain.com). ## 2. Create a `StateGraph` Now you can create a basic chatbot using LangGraph. This chatbot will respond directly to user messages. Start by creating a `StateGraph`. A `StateGraph` object defines the structure of our chatbot as a "state machine". We'll add `nodes` to represent the llm and functions our chatbot can call and `edges` to specify how the bot should transition between these functions. ```python from typing import Annotated from typing_extensions import TypedDict from langgraph.graph import StateGraph, START, END from langgraph.graph.message import add_messages class State(TypedDict): # Messages have the type "list". The `add_messages` function # in the annotation defines how this state key should be updated # (in this case, it appends messages to the list, rather than overwriting them) messages: Annotated[list, add_messages] graph_builder = StateGraph(State) ``` Our graph can now handle two key tasks: 1. Each `node` can receive the current `State` as input and output an update to the state. 2. Updates to `messages` will be appended to the existing list rather than overwriting it, thanks to the prebuilt [`add_messages`](https://langchain-ai.github.io/langgraph/reference/graphs/?h=add+messages#add_messages) function used with the `Annotated` syntax. Concept: When defining a graph, the first step is to define its `State`. The `State` includes the graph's schema and [reducer functions](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers) that handle state updates. In our example, `State` is a `TypedDict` with one key: `messages`. The [`add_messages`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages) reducer function is used to append new messages to the list instead of overwriting it. Keys without a reducer annotation will overwrite previous values. To learn more about state, reducers, and related concepts, see [LangGraph reference docs](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages). ## 3. Add a node Next, add a "`chatbot`" node. **Nodes** represent units of work and are typically regular Python functions. Let's first select a chat model: #### OpenAI ```bash pip install -U "langchain[openai]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["OPENAI_API_KEY"] = "sk-..." llm = init_chat_model("openai:gpt-4.1") ``` ðŸ‘‰ Read the [OpenAI integration docs](https://python.langchain.com/docs/integrations/chat/openai/) #### Anthropic ```bash pip install -U "langchain[anthropic]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["ANTHROPIC_API_KEY"] = "sk-..." llm = init_chat_model("anthropic:claude-3-5-sonnet-latest") ``` ðŸ‘‰ Read the [Anthropic integration docs](https://python.langchain.com/docs/integrations/chat/anthropic/) #### Azure ```bash pip install -U "langchain[openai]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["AZURE_OPENAI_API_KEY"] = "..." os.environ["AZURE_OPENAI_ENDPOINT"] = "..." os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview" llm = init_chat_model( "azure_openai:gpt-4.1", azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"], ) ``` ðŸ‘‰ Read the [Azure integration docs](https://python.langchain.com/docs/integrations/chat/azure_chat_openai/) #### Google Gemini ```bash pip install -U "langchain[google-genai]" ``` ```python import os from langchain.chat_models import init_chat_model os.environ["GOOGLE_API_KEY"] = "..." llm = init_chat_model("google_genai:gemini-2.0-flash") ``` ðŸ‘‰ Read the [Google GenAI integration docs](https://python.langchain.com/docs/integrations/chat/google_generative_ai/) #### AWS Bedrock ```bash pip install -U "langchain[aws]" ``` ```python from langchain.chat_models import init_chat_model # Follow the steps here to configure your credentials: # https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html llm = init_chat_model( "anthropic.claude-3-5-sonnet-20240620-v1:0", model_provider="bedrock_converse", ) ``` ðŸ‘‰ Read the [AWS Bedrock integration docs](https://python.langchain.com/docs/integrations/chat/bedrock/) We can now incorporate the chat model into a simple node: ```python def chatbot(state: State): return {"messages": [llm.invoke(state["messages"])]} # The first argument is the unique node name # The second argument is the function or object that will be called whenever # the node is used. graph_builder.add_node("chatbot", chatbot) ``` **Notice** how the `chatbot` node function takes the current `State` as input and returns a dictionary containing an updated `messages` list under the key "messages". This is the basic pattern for all LangGraph node functions. The `add_messages` function in our `State` will append the LLM's response messages to whatever messages are already in the state. ## 4. Add an `entry` point Add an `entry` point to tell the graph **where to start its work** each time it is run: ```python graph_builder.add_edge(START, "chatbot") ``` ## 5. Add an `exit` point Add an `exit` point to indicate **where the graph should finish execution**. This is helpful for more complex flows, but even in a simple graph like this, adding an end node improves clarity. ```python graph_builder.add_edge("chatbot", END) ``` This tells the graph to terminate after running the chatbot node. ## 6. Compile the graph Before running the graph, we'll need to compile it. We can do so by calling `compile()` on the graph builder. This creates a `CompiledStateGraph` we can invoke on our state. ```python graph = graph_builder.compile() ``` ## 7. Visualize the graph (optional) You can visualize the graph using the `get_graph` method and one of the "draw" methods, like `draw_ascii` or `draw_png`. The `draw` methods each require additional dependencies. ```python from IPython.display import Image, display try: display(Image(graph.get_graph().draw_mermaid_png())) except Exception: # This requires some extra dependencies and is optional pass ``` ![basic chatbot diagram](https://langchain-ai.github.io/langgraph/tutorials/get-started/basic-chatbot.png) ## 8. Run the chatbot Now run the chatbot! Tip: You can exit the chat loop at any time by typing `quit`, `exit`, or `q`. ```python def stream_graph_updates(user_input: str): for event in graph.stream({"messages": [{"role": "user", "content": user_input}]}): for value in event.values(): print("Assistant:", value["messages"][-1].content) while True: try: user_input = input("User: ") if user_input.lower() in ["quit", "exit", "q"]: print("Goodbye!") break stream_graph_updates(user_input) except: # fallback if input() is not available user_input = "What do you know about LangGraph?" print("User: " + user_input) stream_graph_updates(user_input) break ``` ```text Assistant: LangGraph is a library designed to help build stateful multi-agent applications using language models. It provides tools for creating workflows and state machines to coordinate multiple AI agents or language model interactions. LangGraph is built on top of LangChain, leveraging its components while adding graph-based coordination capabilities. It's particularly useful for developing more complex, stateful AI applications that go beyond simple query-response interactions. Goodbye! ``` **Congratulations!** You've built your first chatbot using LangGraph. This bot can engage in basic conversation by taking user input and generating responses using an LLM. You can inspect a [LangSmith Trace](https://smith.langchain.com/public/7527e308-9502-4894-b347-f34385740d5a/r) for the call above. Below is the full code for this tutorial: ```python from typing import Annotated from langchain.chat_models import init_chat_model from typing_extensions import TypedDict from langgraph.graph import StateGraph, START, END from langgraph.graph.message import add_messages class State(TypedDict): messages: Annotated[list, add_messages] graph_builder = StateGraph(State) llm = init_chat_model("anthropic:claude-3-5-sonnet-latest") def chatbot(state: State): return {"messages": [llm.invoke(state["messages"])]} # The first argument is the unique node name # The second argument is the function or object that will be called whenever # the node is used. graph_builder.add_node("chatbot", chatbot) graph_builder.add_edge(START, "chatbot") graph_builder.add_edge("chatbot", END) graph = graph_builder.compile() ``` ## Next steps You may have noticed that the bot's knowledge is limited to what's in its training data. In the next part, we'll [add a web search tool](https://langchain-ai.github.io/langgraph/tutorials/get-started/2-add-tools/) to expand the bot's knowledge and make it more capable.

---

# URL: https://langchain-ai.github.io/langgraph/reference
**Depth**: 1

# Reference Welcome to the LangGraph reference docs! These pages detail the core interfaces you will use when building with LangGraph. Each section covers a different part of the ecosystem. > Tip > If you are just getting started, see [LangGraph basics](https://langchain-ai.github.io/langgraph/concepts/why-langgraph/) for an introduction to the main concepts and usage patterns. ## LangGraph The core APIs for the LangGraph open source library. * [Graphs](https://langchain-ai.github.io/langgraph/reference/graphs/): Main graph abstraction and usage. * [Functional API](https://langchain-ai.github.io/langgraph/reference/func/): Functional programming interface for graphs. * [Pregel](https://langchain-ai.github.io/langgraph/reference/pregel/): Pregel-inspired computation model. * [Checkpointing](https://langchain-ai.github.io/langgraph/reference/checkpoints/): Saving and restoring graph state. * [Storage](https://langchain-ai.github.io/langgraph/reference/store/): Storage backends and options. * [Caching](https://langchain-ai.github.io/langgraph/reference/cache/): Caching mechanisms for performance. * [Types](https://langchain-ai.github.io/langgraph/reference/types/): Type definitions for graph components. * [Config](https://langchain-ai.github.io/langgraph/reference/config/): Configuration options. * [Errors](https://langchain-ai.github.io/langgraph/reference/errors/): Error types and handling. * [Constants](https://langchain-ai.github.io/langgraph/reference/constants/): Global constants. * [Channels](https://langchain-ai.github.io/langgraph/reference/channels/): Message passing and channels. ## Prebuilt components Higher-level abstractions for common workflows, agents, and other patterns. * [Agents](https://langchain-ai.github.io/langgraph/reference/agents/): Built-in agent patterns. * [Supervisor](https://langchain-ai.github.io/langgraph/reference/supervisor/): Orchestration and delegation. * [Swarm](https://langchain-ai.github.io/langgraph/reference/swarm/): Multi-agent collaboration. * [MCP Adapters](https://langchain-ai.github.io/langgraph/reference/mcp/): Integrations with external systems. ## LangGraph Platform Tools for deploying and connecting to the LangGraph Platform. * [CLI](https://langchain-ai.github.io/langgraph/cloud/reference/cli/): Command-line interface for building and deploying LangGraph Platform applications. * [Server API](https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref/): REST API for the LangGraph Server. * [SDK (Python)](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/): Python SDK for interacting with instances of the LangGraph Server. * [SDK (JS/TS)](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/js_ts_sdk_ref/): JavaScript/TypeScript SDK for interacting with instances of the LangGraph Server. * [RemoteGraph](https://langchain-ai.github.io/langgraph/reference/remote_graph/): `Pregel` abstraction for connecting to LangGraph Server instances. * [Environment variables](https://langchain-ai.github.io/langgraph/cloud/reference/env_var/): Supported configuration variables when deploying with the LangGraph Platform.

---

# URL: https://langchain-ai.github.io/langgraph/concepts/low_level
**Depth**: 1

# Graph API concepts At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 1. `State`: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a `TypedDict` or Pydantic `BaseModel`. 2. `Nodes`: Python functions that encode the logic of your agents. They receive the current `State` as input, perform some computation or side-effect, and return an updated `State`. 3. `Edges`: Python functions that determine which `Node` to execute next based on the current `State`. They can be conditional branches or fixed transitions. By composing `Nodes` and `Edges`, you can create complex, looping workflows that evolve the `State` over time. The real power, though, comes from how LangGraph manages that `State`. To emphasize: `Nodes` and `Edges` are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: *nodes do the work, edges tell what to do next*. LangGraph's underlying graph algorithm uses [message passing](https://en.wikipedia.org/wiki/Message_passing) to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's [Pregel](https://research.google/pubs/pregel-a-system-for-large-scale-graph-processing/) system, the program proceeds in discrete "super-steps." A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an `inactive` state. A node becomes `active` when it receives a new message (state) on any of its incoming edges (or "channels"). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to `halt` by marking themselves as `inactive`. The graph execution terminates when all nodes are `inactive` and no messages are in transit. ## StateGraph The `StateGraph` class is the main graph class to use. This is parameterized by a user defined `State` object. ## Compiling your graph To build your graph, you first define the [state](#state), you then add [nodes](#nodes) and [edges](#edges), and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like [checkpointers](https://langchain-ai.github.io/langgraph/concepts/persistence/) and breakpoints. You compile your graph by just calling the `.compile` method: ```python graph = graph_builder.compile(...) ``` You **MUST** compile your graph before you can use it. ## State The first thing you do when you define a graph is define the `State` of the graph. The `State` consists of the [schema of the graph](#schema) as well as [`reducer` functions](#reducers) which specify how to apply updates to the state. The schema of the `State` will be the input schema to all `Nodes` and `Edges` in the graph, and can be either a `TypedDict` or a `Pydantic` model. All `Nodes` will emit updates to the `State` which are then applied using the specified `reducer` function. ### Schema The main documented way to specify the schema of a graph is by using `TypedDict`. However, we also support [using a Pydantic BaseModel](https://langchain-ai.github.io/langgraph/how-tos/graph-api/#use-pydantic-models-for-graph-state) as your graph state to add **default values** and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the [guide here](https://langchain-ai.github.io/langgraph/how-tos/graph-api/#define-input-and-output-schemas) for how to use. #### Multiple schemas Typically, all graph nodes communicate with a single schema. This means that they will read and write to the same state channels. But, there are cases where we want more control over this: * Internal nodes can pass information that is not required in the graph's input / output. * We may also want to use different input / output schemas for the graph. The output might, for example, only contain a single relevant output key. It is possible to have nodes write to private state channels inside the graph for internal node communication. We can simply define a private schema, `PrivateState`. See [this guide](https://langchain-ai.github.io/langgraph/how-tos/graph-api/#pass-private-state-between-nodes) for more detail. It is also possible to define explicit input and output schemas for a graph. In these cases, we define an "internal" schema that contains *all* keys relevant to graph operations. But, we also define `input` and `output` schemas that are sub-sets of the "internal" schema to constrain the input and output of the graph. See [this guide](https://langchain-ai.github.io/langgraph/how-tos/graph-api/#define-input-and-output-schemas) for more detail. Let's look at an example: ```python class InputState(TypedDict): user_input: str class OutputState(TypedDict): graph_output: str class OverallState(TypedDict): foo: str user_input: str graph_output: str class PrivateState(TypedDict): bar: str def node_1(state: InputState) -> OverallState: # Write to OverallState return {"foo": state["user_input"] + " name"} def node_2(state: OverallState) -> PrivateState: # Read from OverallState, write to PrivateState return {"bar": state["foo"] + " is"} def node_3(state: PrivateState) -> OutputState: # Read from PrivateState, write to OutputState return {"graph_output": state["bar"] + " Lance"} builder = StateGraph(OverallState,input_schema=InputState,output_schema=OutputState) builder.add_node("node_1", node_1) builder.add_node("node_2", node_2) builder.add_node("node_3", node_3) builder.add_edge(START, "node_1") builder.add_edge("node_1", "node_2") builder.add_edge("node_2", "node_3") builder.add_edge("node_3", END) graph = builder.compile() graph.invoke({"user_input":"My"}) {'graph_output': 'My name is Lance'} ``` There are two subtle and important points to note here: 1. We pass `state: InputState` as the input schema to `node_1`. But, we write out to `foo`, a channel in `OverallState`. How can we write out to a state channel that is not included in the input schema? This is because a node *can write to any state channel in the graph state.* The graph state is the union of the state channels defined at initialization, which includes `OverallState` and the filters `InputState` and `OutputState`. 2. We initialize the graph with `StateGraph(OverallState,input_schema=InputState,output_schema=OutputState)`. So, how can we write to `PrivateState` in `node_2`? How does the graph gain access to this schema if it was not passed in the `StateGraph` initialization? We can do this because *nodes can also declare additional state channels* as long as the state schema definition exists. In this case, the `PrivateState` schema is defined, so we can add `bar` as a new state channel in the graph and write to it. ### Reducers Reducers are key to understanding how updates from nodes are applied to the `State`. Each key in the `State` has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: #### Default Reducer These two examples show how to use the default reducer: **Example A:** ```python from typing_extensions import TypedDict class State(TypedDict): foo: int bar: list[str] ``` In this example, no reducer functions are specified for any key. Let's assume the input to the graph is `{"foo": 1, "bar": ["hi"]}`. Let's then assume the first `Node` returns `{"foo": 2}`. This is treated as an update to the state. Notice that the `Node` does not need to return the whole `State` schema - just an update. After applying this update, the `State` would then be `{"foo": 2, "bar": ["hi"]}`. If the second node returns `{"bar": ["bye"]}` then the `State` would then be `{"foo": 2, "bar": ["bye"]}` **Example B:** ```python from typing import Annotated from typing_extensions import TypedDict from operator import add class State(TypedDict): foo: int bar: Annotated[list[str], add] ``` In this example, we've used the `Annotated` type to specify a reducer function (`operator.add`) for the second key (`bar`). Note that the first key remains unchanged. Let's assume the input to the graph is `{"foo": 1, "bar": ["hi"]}`. Let's then assume the first `Node` returns `{"foo": 2}`. This is treated as an update to the state. Notice that the `Node` does not need to return the whole `State` schema - just an update. After applying this update, the `State` would then be `{"foo": 2, "bar": ["hi"]}`. If the second node returns `{"bar": ["bye"]}` then the `State` would then be `{"foo": 2, "bar": ["hi", "bye"]}`. Notice here that the `bar` key is updated by adding the two lists together. ### Working with Messages in Graph State #### Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's [`ChatModel`](https://python.langchain.com/docs/concepts/#chat-models) in particular accepts a list of `Message` objects as inputs. These messages come in a variety of forms such as `HumanMessage` (user input) or `AIMessage` (LLM response). To read more about what message objects are, please refer to [this](https://python.langchain.com/docs/concepts/#messages) conceptual guide. #### Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of `Message` objects and annotate it with a reducer function (see `messages` key in the example below). The reducer function is vital to telling the graph how to update the list of `Message` objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use `operator.add` as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use `operator.add`, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt `add_messages` function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. #### Serialization In addition to keeping track of message IDs, the `add_messages` function will also try to deserialize messages into LangChain `Message` objects whenever a state update is received on the `messages` channel. See more information on LangChain serialization/deserialization [here](https://python.langchain.com/docs/how_to/serialization/). This allows sending graph inputs / state updates in the following format: ```python # this is supported {"messages": [HumanMessage(content="message")]} # and this is also supported {"messages": [{"type": "human", "content": "message"}]} ``` Since the state updates are always deserialized into LangChain `Messages` when using `add_messages`, you should use dot notation to access message attributes, like `state["messages"][-1].content`. Below is an example of a graph that uses `add_messages` as its reducer function. *API Reference: [AnyMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.AnyMessage.html) | [add\\_messages](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages)* ```python from langchain_core.messages import AnyMessage from langgraph.graph.message import add_messages from typing import Annotated from typing_extensions import TypedDict class GraphState(TypedDict): messages: Annotated[list[AnyMessage], add_messages] ``` #### MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called `MessagesState` which makes it easy to use messages. `MessagesState` is defined with a single `messages` key which is a list of `AnyMessage` objects and uses the `add_messages` reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: ```python from langgraph.graph import MessagesState class State(MessagesState): documents: list[str] ``` ## Nodes In LangGraph, nodes are typically python functions (sync or async) where the **first** positional argument is the [state](#state), and (optionally), the **second** positional argument is a "config", containing optional [configurable parameters](#configuration) (such as a `thread_id`). Similar to `NetworkX`, you add these nodes to a graph using the [`add_node`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph.add_node) method: *API Reference: [RunnableConfig](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)* ```python from typing_extensions import TypedDict from langchain_core.runnables import RunnableConfig from langgraph.graph import StateGraph class State(TypedDict): input: str results: str builder = StateGraph(State) def my_node(state: State, config: RunnableConfig): print("In node: ", config["configurable"]["user_id"]) return {"results": f"Hello, {state['input']}!"} # The second argument is optional def my_other_node(state: State): return state builder.add_node("my_node", my_node) builder.add_node("other_node", my_other_node) ... ``` Behind the scenes, functions are converted to [RunnableLambda](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.base.RunnableLambda.html#langchain_core.runnables.base.RunnableLambda)s, which add batch and async support to your function, along with native tracing and debugging. If you add a node to a graph without specifying a name, it will be given a default name equivalent to the function name. ```python builder.add_node(my_node) # You can then create edges to/from this node by referencing it as `"my_node"` ``` ### `START` Node The `START` Node is a special node that represents the node that sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. *API Reference: [START](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START)* ```python from langgraph.graph import START graph.add_edge(START, "node_a") ``` ### `END` Node The `END` Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. ```python from langgraph.graph import END graph.add_edge("node_a", END) ``` ### Node Caching LangGraph supports caching of tasks/nodes based on the input to the node. To use caching: * Specify a cache when compiling a graph (or specifying an entrypoint) * Specify a cache policy for nodes. Each cache policy supports: * `key_func` used to generate a cache key based on the input to a node, which defaults to a `hash` of the input with pickle. * `ttl`, the time to live for the cache in seconds. If not specified, the cache will never expire. For example: *API Reference: [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)* ```python import time from typing_extensions import TypedDict from langgraph.graph import StateGraph from langgraph.cache.memory import InMemoryCache from langgraph.types import CachePolicy class State(TypedDict): x: int result: int builder = StateGraph(State) def expensive_node(state: State) -> dict[str, int]: # expensive computation time.sleep(2) return {"result": state["x"] * 2} builder.add_node("expensive_node", expensive_node, cache_policy=CachePolicy(ttl=3)) builder.set_entry_point("expensive_node") builder.set_finish_point("expensive_node") graph = builder.compile(cache=InMemoryCache()) print(graph.invoke({"x": 5}, stream_mode='updates')) #[{'expensive_node': {'result': 10}}] print(graph.invoke({"x": 5}, stream_mode='updates')) #[{'expensive_node': {'result': 10}, '__metadata__': {'cached': True}}] ``` ## Edges Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: * Normal Edges: Go directly from one node to the next. * Conditional Edges: Call a function to determine which node(s) to go to next. * Entry Point: Which node to call first when user input arrives. * Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives. A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, **all** of those destination nodes will be executed in parallel as a part of the next superstep. ### Normal Edges If you **always** want to go from node A to node B, you can use the [`add_edge`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph.add_edge) method directly. ```python graph.add_edge("node_a", "node_b") ``` ### Conditional Edges If you want to **optionally** route to 1 or more edges (or optionally terminate), you can use the [`add_conditional_edges`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph.add_conditional_edges) method. This method accepts the name of a node and a "routing function" to call after that node is executed: ```python graph.add_conditional_edges("node_a", routing_function) ``` Similar to nodes, the `routing_function` accepts the current `state` of the graph and returns a value. By default, the return value `routing_function` is used as the name of the node (or list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the `routing_function`'s output to the name of the next node. ```python graph.add_conditional_edges("node_a", routing_function, {True: "node_b", False: "node_c"}) ``` > Tip > > Use [`Command`](#command) instead of conditional edges if you want to combine state updates and routing in a single function. ### Entry Point The entry point is the first node(s) that are run when the graph starts. You can use the [`add_edge`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph.add_edge) method from the virtual [`START`](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START) node to the first node to execute to specify where to enter the graph. *API Reference: [START](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START)* ```python from langgraph.graph import START graph.add_edge(START, "node_a") ``` ### Conditional Entry Point A conditional entry point lets you start at different nodes depending on custom logic. You can use [`add_conditional_edges`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph.add_conditional_edges) from the virtual [`START`](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START) node to accomplish this. *API Reference: [START](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START)* ```python from langgraph.graph import START graph.add_conditional_edges(START, routing_function) ``` You can optionally provide a dictionary that maps the `routing_function`'s output to the name of the next node. ```python graph.add_conditional_edges(START, routing_function, {True: "node_b", False: "node_c"}) ``` ## `Send` By default, `Nodes` and `Edges` are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of `State` to exist at the same time. A common example of this is with [map-reduce](https://langchain-ai.github.io/langgraph/how-tos/map-reduce/) design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input `State` to the downstream `Node` should be different (one for each generated object). To support this design pattern, LangGraph supports returning [`Send`](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Send) objects from conditional edges. `Send` takes two arguments: first is the name of the node, and second is the state to pass to that node. ```python def continue_to_jokes(state: OverallState): return [Send("generate_joke", {"subject": s}) for s in state['subjects']] graph.add_conditional_edges("node_a", continue_to_jokes) ``` ## `Command` It can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a [`Command`](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Command) object from node functions: ```python def my_node(state: State) -> Command[Literal["my_other_node"]]: return Command( # state update update={"foo": "bar"}, # control flow goto="my_other_node" ) ``` With `Command` you can also achieve dynamic control flow behavior (identical to [conditional edges](#conditional-edges)): ```python def my_node(state: State) -> Command[Literal["my_other_node"]]: if state["foo"] == "bar": return Command(update={"foo": "baz"}, goto="my_other_node") ``` > Important > > When returning `Command` in your node functions, you must add return type annotations with the list of node names the node is routing to, e.g. `Command[Literal["my_other_node"]]`. This is necessary for the graph rendering and tells LangGraph that `my_node` can navigate to `my_other_node`. Check out this [how-to guide](https://langchain-ai.github.io/langgraph/how-tos/graph-api/#combine-control-flow-and-state-updates-with-command) for an end-to-end example of how to use `Command`. ### When should I use Command instead of conditional edges? Use `Command` when you need to **both** update the graph state **and** route to a different node. For example, when implementing [multi-agent handoffs](https://langchain-ai.github.io/langgraph/concepts/multi_agent/#handoffs) where it's important to route to a different agent and pass some information to that agent. Use [conditional edges](#conditional-edges) to route between nodes conditionally without updating the state. ### Navigating to a node in a parent graph If you are using [subgraphs](https://langchain-ai.github.io/langgraph/concepts/subgraphs/), you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify `graph=Command.PARENT` in `Command`: ```python def my_node(state: State) -> Command[Literal["other_subgraph"]]: return Command( update={"foo": "bar"}, goto="other_subgraph", # where `other_subgraph` is a node in the parent graph graph=Command.PARENT ) ``` > Note > > Setting `graph` to `Command.PARENT` will navigate to the closest parent graph. > State updates with `Command.PARENT` > > When you send updates from a subgraph node to a parent graph node for a key that's shared by both parent and subgraph [state schemas](#schema), you **must** define a [reducer](#reducers) for the key you're updating in the parent graph state. See this [example](https://langchain-ai.github.io/langgraph/how-tos/graph-api/#navigate-to-a-node-in-a-parent-graph). This is particularly useful when implementing [multi-agent handoffs](https://langchain-ai.github.io/langgraph/concepts/multi_agent/#handoffs). Check out [this guide](https://langchain-ai.github.io/langgraph/how-tos/graph-api/#navigate-to-a-node-in-a-parent-graph) for detail. ### Using inside tools A common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation. Refer to [this guide](https://langchain-ai.github.io/langgraph/how-tos/graph-api/#use-inside-tools) for detail. ### Human-in-the-loop `Command` is an important part of human-in-the-loop workflows: when using `interrupt()` to collect user input, `Command` is then used to supply the input and resume execution via `Command(resume="User input")`. Check out [this conceptual guide](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/) for more information. ## Graph Migrations LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. * For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc) * For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution. * For modifying state, we have full backwards and forwards compatibility for adding and removing keys * State keys that are renamed lose their saved state in existing threads * State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution. ## Configuration When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single "cognitive architecture" (the graph) but have multiple different instance of it. You can optionally specify a `config_schema` when creating a graph. ```python class ConfigSchema(TypedDict): llm: str graph = StateGraph(State, config_schema=ConfigSchema) ``` You can then pass this configuration into the graph using the `configurable` config field. ```python config = {"configurable": {"llm": "anthropic"}} graph.invoke(inputs, config=config) ``` You can then access and use this configuration inside a node or conditional edge: ```python def node_a(state, config): llm_type = config.get("configurable", {}).get("llm", "openai") llm = get_llm(llm_type) ... ``` See [this guide](https://langchain-ai.github.io/langgraph/how-tos/graph-api/#add-runtime-configuration) for a full breakdown on configuration. ### Recursion Limit The recursion limit sets the maximum number of [super-steps](#graphs) the graph can execute during a single execution. Once the limit is reached, LangGraph will raise `GraphRecursionError`. By default this value is set to 25 steps. The recursion limit can be set on any graph at runtime, and is passed to `.invoke`/.stream` via the config dictionary. Importantly, `recursion_limit` is a standalone `config` key and should not be passed inside the `configurable` key as all other user-defined configuration. See the example below: ```python graph.invoke(inputs, config={"recursion_limit": 5, "configurable":{"llm": "anthropic"}}) ``` Read [this how-to](https://langchain-ai.github.io/langgraph/how-tos/recursion-limit/) to learn more about how the recursion limit works. ## Visualization It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See [this how-to guide](https://langchain-ai.github.io/langgraph/how-tos/graph-api/#visualize-your-graph) for more info.

---

